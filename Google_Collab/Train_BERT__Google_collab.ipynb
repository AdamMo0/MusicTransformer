{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Project: Music Composition with BERT\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we explore the fascinating world of music composition using state-of-the-art natural language processing models, specifically BERT (Bidirectional Encoder Representations from Transformers). The goal is to train the model to generate music compositions in ABC notation.\n",
    "\n",
    "## Project Workflow\n",
    "\n",
    "### Data Loading and Preparation\n",
    "\n",
    "- **Data Source**: The project begins by obtaining a dataset of music compositions in ABC notation. This dataset contains the music pieces that we'll use for training our models.\n",
    "\n",
    "- **Data Preprocessing**: The dataset is preprocessed to clean and format the ABC notation for model training. This includes tokenization and encoding into a suitable format for the models.\n",
    "\n",
    "### BERT Model Training\n",
    "\n",
    "\n",
    "- **Model Selection**: We also train a BERT model, adapted for music generation, using PyTorch.\n",
    "\n",
    "- **Hyperparameter Tuning**: The training process includes hyperparameter tuning to find the optimal combination of settings for BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5n6YGv1LPysa",
    "outputId": "9150f4d3-e6c2-4b13-bcb6-5e641927b8ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.15.11-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=d506834138c71cd24ea30e460873ecc99a4e53f30c635de31d97b063917a0721\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.37 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.2 smmap-5.0.1 wandb-0.15.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtokentome\n",
      "  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m81.9/86.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.10/dist-packages (from youtokentome) (8.1.7)\n",
      "Building wheels for collected packages: youtokentome\n",
      "  Building wheel for youtokentome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=1948600 sha256=814a8d6cbc3fc708c7399f01decf0c7e62c359bb09f6a5e8a12b7b327bff4d9e\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n",
      "Successfully built youtokentome\n",
      "Installing collected packages: youtokentome\n",
      "Successfully installed youtokentome-1.0.6\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.23.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "wandb.login(key='30b44f6f59b06faebb3d1f78df32c6fd9961f07d')\n",
    "!{sys.executable} -m pip install youtokentome\n",
    "!{sys.executable} -m pip install transformers\n",
    "!pip install accelerate -U\n",
    "from transformers import Trainer, TrainingArguments,default_data_collator\n",
    "import youtokentome as yttm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7WRPXEt6VHI",
    "outputId": "83445213-b72b-449e-d56a-119f8f6bd846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "ORIGIN = os.path.normpath(os.getcwd())\n",
    "print(ORIGIN)\n",
    "TRAIN_DIR =\"/content/drive/MyDrive/test2/\"\n",
    "VALID_DIR = \"/content/drive/MyDrive/Music_project/valid_path/\"\n",
    "TEST_DIR = \"/content/drive/MyDrive/Music_project/test_path/\"\n",
    "TOKENIZER_DIR = \"/content/drive/MyDrive/Music_project/abc_run5.yttm\"\n",
    "DATASET_DIR =\"/content/drive/MyDrive/Music_project/300,000_new_samples.csv\"\n",
    "# OUTPUT_DIR = \"/content/drive/MyDrive/Music_project/output_GPT2_checkpoints6\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/Music_project/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8EzDqCVfBHb",
    "outputId": "0ab2a582-6178-4731-8185-115adb0fe84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = yttm.BPE(TOKENIZER_DIR) # import the trained tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
    "\n",
    "def get_model(vocab_size=30000):\n",
    "    config_encoder = BertConfig()\n",
    "    config_decoder = BertConfig()\n",
    "\n",
    "    config_encoder.vocab_size = vocab_size\n",
    "    config_decoder.vocab_size = vocab_size\n",
    "\n",
    "    config_decoder.is_decoder = True\n",
    "    config_decoder.add_cross_attention = True\n",
    "\n",
    "    config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "    model = EncoderDecoderModel(config=config)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(vocab_size=tokenizer.vocab_size()) # load the BERT model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wHIagjynTcrQ"
   },
   "outputs": [],
   "source": [
    "USEABLE_PARAMS = [i+\":\" for i in \"BCDFGHIKLMmNOPQRrSsTUVWwXZ\"] # These are the parameters for key\n",
    "\n",
    "def read_abc(path):\n",
    "    keys = []\n",
    "    notes = []\n",
    "    with open(path) as rf:\n",
    "        for line in rf:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"%\"): # Skip any commments\n",
    "                continue\n",
    "\n",
    "            if any([line.startswith(key) for key in USEABLE_PARAMS]):\n",
    "                if(line.startswith('T')):\n",
    "                    continue # skipping the title for better tokenization\n",
    "#                 if(line.startswith('L')):\n",
    "#                     print(line) ## Checking all L in all files\n",
    "                # After checking the all midi files, they all have the length (L) : 1/8\n",
    "                keys.append(line)\n",
    "            else:\n",
    "                notes.append(line)\n",
    "\n",
    "    keys = \" \".join(keys)\n",
    "\n",
    "    notes = \"\".join(notes).strip()\n",
    "    notes = notes.replace(\" \", \"\")\n",
    "\n",
    "    if notes.endswith(\"|\"):\n",
    "        notes = notes[:-1]\n",
    "    # Remove unneeded character.\n",
    "    notes = notes.replace(\" \\ \", \"\")\n",
    "    notes = notes.replace(\"\\\\\", \"\")\n",
    "    notes = notes.replace(\"\\ \", \"\")\n",
    "    notes = notes.replace(\"x8|\", \"\") # 8 because all of the midi file has a L:1/8 that means one muted bar\n",
    "    notes = notes.replace(\"z8|\", \"\") # 8 because all of the midi file has a L:1/8 that means one muted bar\n",
    "\n",
    "    notes = notes.strip()\n",
    "    notes = \" \".join(notes.split(\" \"))\n",
    "\n",
    "    if not keys or not notes:\n",
    "        return None, None\n",
    "\n",
    "    return keys, notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lJT3Gr0WmRl",
    "outputId": "204fdecb-ab6b-4864-c7ec-6c668445cc3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "77IoANOAQS2X",
    "outputId": "dacf1279-ed69-4f21-f1d2-654ac2e8453d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Music_project/output_BERT_checkpoints6'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jLugMQ6XO2n"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "  data = []\n",
    "  count = 0\n",
    "  counter = 0\n",
    "  directory_files = os.listdir(path)\n",
    "  directory_path = path\n",
    "\n",
    "  for file in directory_files:\n",
    "      filename = os.path.join(directory_path, file)\n",
    "      print(filename)\n",
    "      keys, notes = read_abc(filename)\n",
    "      print(\"======================\")\n",
    "      print(keys)\n",
    "      print(notes)\n",
    "      if keys is None:\n",
    "          continue\n",
    "\n",
    "      keys_tokens = tokenizer.encode(keys)\n",
    "\n",
    "\n",
    "      bars = notes.split(\",\")\n",
    "      input_bars = []\n",
    "      target_bars = []\n",
    "      count = 0\n",
    "      notes_tokens = [tokenizer.encode(i + \" | \") for i in bars]\n",
    "\n",
    "\n",
    "      print(\"======total=====\")\n",
    "\n",
    "      print(notes_tokens)\n",
    "\n",
    "      sequence_len = sum(len(i) for i in notes_tokens)\n",
    "\n",
    "      counter = counter+1\n",
    "      if counter == 10:\n",
    "        break\n",
    "      data.append((keys_tokens, notes_tokens))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWR6wMdYQ29w",
    "outputId": "9268b717-e7cc-432a-c36c-ddbc4a00e6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/test2/8352_9782.abc\n",
      "======total=====\n",
      "[[58, 33, 18, 14, 17, 38, 18, 14, 17, 12, 35, 18, 14, 17, 12, 38, 11, 18, 14, 17, 12, 35, 11, 18, 14, 17, 12, 7131, 38, 14, 17, 35, 14, 17, 38, 11, 14, 17, 35, 11, 14, 17, 12, 7131, 33, 14, 17, 38, 14, 17, 35, 14, 17, 35, 11, 14, 17, 12, 60, 35, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 35, 11, 14, 17, 12, 60, 35, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 35, 11, 14, 17, 12, 60, 35, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 35, 11, 14, 17, 12, 60, 35, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 35, 11, 14, 17, 12, 60, 35, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 35, 11, 14, 17, 12, 60, 35, 11, 14, 17, 12, 930, 220], [58, 33, 17, 38, 17, 35, 17, 38, 11, 17, 12, 35, 11, 17, 12, 7131, 33, 21, 38, 21, 35, 21, 38, 11, 21, 35, 11, 21, 12, 60, 930, 220], [58, 28, 32, 17, 37, 17, 35, 17, 35, 11, 17, 7131, 32, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 7131, 32, 14, 17, 37, 14, 17, 7131, 32, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 7131, 32, 14, 17, 37, 14, 17, 7131, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 14, 17, 58, 33, 14, 17, 12, 32, 14, 17, 38, 14, 17, 12, 37, 14, 17, 35, 14, 17, 7131, 33, 14, 17, 38, 14, 17, 7131, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 14, 17, 58, 32, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 7131, 32, 14, 17, 37, 14, 17, 60, 930, 220], [58, 33, 18, 14, 17, 32, 18, 14, 17, 12, 38, 18, 14, 17, 37, 18, 14, 17, 12, 35, 18, 14, 17, 12, 35, 11, 18, 14, 17, 12, 7131, 32, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 12, 35, 11, 14, 17, 12, 7131, 4339, 12, 21713, 12, 35, 12, 35, 12095, 7131, 32, 12, 37, 12, 16458, 12095, 7131, 32, 19, 37, 19, 12, 35, 19, 35, 11, 19, 60, 930, 220], [58, 61, 32, 17, 37, 17, 34, 17, 37, 11, 17, 7131, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 930, 220], [58, 61, 32, 19, 37, 19, 34, 19, 37, 11, 19, 12, 7131, 61, 32, 19, 37, 19, 34, 19, 37, 11, 19, 60, 930, 220], [58, 33, 17, 37, 17, 35, 17, 33, 9832, 17, 12, 7131, 33, 14, 17, 37, 14, 17, 35, 14, 17, 33, 9832, 14, 17, 12, 60, 33, 9832, 14, 17, 49146, 33, 14, 17, 37, 14, 17, 35, 14, 17, 33, 9832, 14, 17, 12, 60, 33, 9832, 14, 17, 58, 33, 14, 17, 37, 14, 17, 35, 14, 17, 34, 11, 14, 17, 12, 60, 34, 11, 14, 17, 49146, 33, 14, 17, 37, 14, 17, 35, 14, 17, 34, 11, 14, 17, 12, 60, 34, 11, 14, 17, 49146, 33, 14, 17, 37, 14, 17, 35, 14, 17, 34, 11, 14, 17, 12, 60, 34, 11, 14, 17, 49146, 33, 14, 17, 37, 14, 17, 35, 14, 17, 34, 11, 14, 17, 12, 60, 34, 11, 14, 17, 930, 220], [58, 33, 17, 37, 17, 35, 17, 35, 11, 17, 12, 33, 9832, 17, 12, 7131, 33, 17, 12, 37, 17, 12, 35, 17, 12, 35, 11, 17, 33, 9832, 17, 12, 7131, 33, 17, 12, 37, 17, 12, 35, 17, 12, 36, 11, 17, 33, 9832, 17, 12, 7131, 33, 17, 37, 17, 35, 17, 37, 11, 17, 33, 9832, 17, 60, 930, 220], [58, 33, 17, 38, 17, 35, 17, 38, 11, 17, 12, 7131, 33, 14, 17, 38, 14, 17, 35, 14, 17, 38, 11, 14, 17, 12, 60, 38, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 38, 11, 14, 17, 12, 60, 38, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 38, 11, 14, 17, 12, 60, 38, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 38, 11, 14, 17, 12, 60, 38, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 38, 11, 14, 17, 12, 60, 38, 11, 14, 17, 49146, 33, 14, 17, 38, 14, 17, 35, 14, 17, 38, 11, 14, 17, 12, 60, 38, 11, 14, 17, 12, 930, 220], [58, 33, 17, 38, 17, 35, 17, 38, 11, 17, 12, 7131, 33, 21, 38, 21, 35, 21, 38, 11, 21, 60, 930, 220], [58, 28, 32, 12, 37, 12, 35, 12, 35, 12095, 7131, 33, 12, 4760, 12, 37, 16458, 11, 7131, 33, 14, 17, 32, 14, 17, 38, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 14, 17, 58, 33, 14, 17, 12, 32, 14, 17, 38, 14, 17, 12, 37, 14, 17, 35, 14, 17, 7131, 33, 14, 17, 12, 38, 14, 17, 12, 7131, 33, 14, 17, 12, 32, 14, 17, 38, 14, 17, 12, 37, 14, 17, 35, 14, 17, 7131, 33, 14, 17, 12, 38, 14, 17, 12, 7131, 33, 14, 17, 12, 32, 14, 17, 12, 38, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 7131, 33, 14, 17, 12, 32, 14, 17, 12, 38, 14, 17, 12, 37, 14, 17, 12, 7131, 33, 14, 17, 32, 14, 17, 38, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 14, 17, 58, 32, 14, 17, 37, 14, 17, 35, 14, 17, 12, 60, 35, 14, 17, 930, 220], [58, 32, 12, 37, 12, 35, 12, 32, 11, 35, 12095, 7131, 32, 17, 12, 37, 17, 12, 35, 17, 12, 35, 11, 17, 12, 7131, 33, 12, 32, 12, 38, 12, 37, 12, 16458, 12095, 7131, 33, 14, 17, 32, 14, 17, 12, 38, 14, 17, 37, 14, 17, 12, 35, 14, 17, 12, 35, 11, 14, 17, 12, 7131, 32, 18, 12, 37, 18, 12, 35, 18, 12, 35, 11, 18, 12, 7131, 32, 14, 17, 37, 14, 17, 35, 14, 17, 35, 11, 14, 17, 60, 930, 220], [58, 33, 17, 38, 17, 36, 17, 36, 11, 17, 7131, 33, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 58, 33, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 58, 33, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 58, 33, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 58, 33, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 58, 33, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 930, 220], [58, 33, 19, 38, 19, 36, 19, 36, 11, 19, 12, 7131, 33, 19, 38, 19, 36, 19, 36, 11, 19, 60, 930, 220], [58, 61, 32, 17, 37, 17, 34, 17, 37, 11, 17, 7131, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 14, 17, 930, 220], [58, 37, 14, 17, 12, 34, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 61, 32, 18, 12, 37, 18, 12, 34, 18, 12, 37, 11, 18, 12, 7131, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 37, 11, 14, 17, 12, 7131, 61, 32, 19, 37, 19, 34, 19, 37, 11, 19, 60, 930, 220]]\n",
      "/content/drive/MyDrive/test2/8425_9782.abc\n",
      "======total=====\n",
      "[[58, 33, 11, 17, 12, 28, 38, 11, 17, 12, 7131, 11012, 12095, 38, 12095, 7131, 33, 12095, 38, 12095, 7131, 4579, 12095, 38, 12095, 7131, 35, 12, 33, 12095, 38, 12095, 7131, 11012, 12095, 38, 12095, 7131, 33, 12095, 38, 12095, 60, 930, 220], [58, 33, 17, 12, 33, 11, 17, 12, 38, 11, 17, 12, 7131, 33, 12, 33, 12095, 38, 12095, 7131, 15199, 12095, 38, 12095, 7131, 33, 11, 17, 12, 32, 11, 17, 38, 11, 17, 12, 7131, 35, 12, 33, 12095, 38, 12095, 7131, 11012, 11, 38, 11, 60, 930, 220], [58, 35, 17, 32, 11, 17, 12, 37, 11, 17, 12, 35, 11, 17, 12, 7131, 3838, 12095, 37, 12095, 35, 12095, 7131, 7708, 12095, 37, 12095, 35, 12095, 7131, 5631, 12095, 37, 12095, 35, 12095, 7131, 67, 32, 12095, 37, 12095, 35, 12095, 7131, 7708, 12095, 37, 12095, 35, 12095, 7131, 5631, 12095, 37, 12095, 35, 11, 60, 930, 220], [58, 32, 11, 17, 12, 37, 11, 17, 12, 7131, 5631, 12095, 37, 12095, 7131, 7708, 11, 37, 12095, 7131, 37, 11, 17, 12, 35, 11, 17, 12, 7131, 8068, 12095, 35, 12095, 7131, 5777, 11, 35, 11, 60, 930, 220], [58, 38, 11, 17, 12, 36, 11, 17, 12, 7131, 33, 11, 38, 12095, 36, 12095, 7131, 35, 38, 12095, 36, 12095, 7131, 33, 11, 38, 12095, 36, 12095, 7131, 11190, 12095, 36, 12095, 7131, 33, 11, 38, 12095, 36, 12095, 7131, 11190, 12095, 36, 12095, 60, 930, 220], [58, 33, 17, 12, 38, 11, 17, 12, 36, 11, 17, 12, 33, 9832, 17, 12, 7131, 33, 12, 33, 12095, 38, 12095, 36, 12095, 33, 9832, 12, 7131, 33, 12, 35, 12, 33, 11, 38, 11, 36, 11, 33, 9832, 7131, 33, 12, 35, 61, 32, 12095, 37, 12095, 33, 9832, 12, 7131, 33, 12, 61, 32, 12095, 37, 12095, 33, 9832, 12, 7131, 33, 12, 33, 11, 61, 32, 12095, 37, 12095, 33, 9832, 12, 7131, 14529, 61, 32, 11, 37, 11, 33, 9832, 60, 930, 220], [58, 61, 32, 11, 17, 12, 37, 11, 17, 12, 7131, 61, 32, 12095, 37, 12095, 7131, 36, 61, 32, 12095, 37, 12095, 7131, 61, 32, 12095, 37, 12095, 7131, 37, 12, 61, 32, 11, 37, 12095, 7131, 15112, 12, 61, 32, 12095, 37, 12095, 7131, 36, 61, 32, 11, 37, 11, 60, 930, 220], [58, 66, 17, 12, 33, 11, 17, 12, 61, 38, 11, 17, 12, 7131, 66, 12, 33, 12095, 61, 38, 12095, 7131, 66, 12, 33, 11, 61, 38, 11, 7131, 66, 17, 12, 34, 17, 12, 61, 32, 11, 17, 12, 7131, 66, 12, 34, 12, 61, 32, 12095, 7131, 66, 34, 61, 32, 11, 60, 930, 220], [58, 37, 21, 12, 35, 21, 12, 33, 11, 21, 12, 7131, 37, 12, 35, 12, 33, 12095, 7131, 69, 14, 17, 66, 14, 17, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 12, 33, 11, 14, 17, 12, 7131, 35, 14, 17, 12, 33, 11, 14, 17, 12, 60, 930, 220], [58, 69, 19, 12, 67, 19, 12, 33, 19, 12, 37, 19, 35, 19, 12, 33, 11, 19, 12, 7131, 69, 18, 67, 18, 33, 18, 37, 18, 12, 35, 18, 12, 33, 11, 18, 12, 7131, 37, 11012, 11, 60, 930, 220], [58, 37, 17, 12, 34, 17, 12, 61, 32, 11, 17, 12, 7131, 37, 12, 34, 12, 61, 32, 12095, 7131, 37, 17, 12, 34, 17, 12, 61, 32, 11, 17, 12, 7131, 16072, 61, 8579, 12, 34, 12, 61, 32, 12095, 7131, 37, 12, 34, 12, 61, 32, 12095, 7131, 69, 14, 17, 66, 14, 17, 61, 32, 14, 17, 37, 14, 17, 34, 14, 17, 12, 61, 32, 11, 14, 17, 12, 7131, 34, 14, 17, 12, 61, 32, 11, 14, 17, 12, 60, 930, 220], [58, 69, 17, 12, 66, 17, 12, 61, 32, 17, 12, 37, 17, 12, 34, 17, 12, 61, 32, 11, 17, 12, 7131, 69, 12, 66, 12, 61, 32, 12, 37, 12, 34, 12, 61, 32, 12095, 7131, 69, 12, 66, 12, 61, 32, 12, 4851, 12, 61, 32, 12095, 7131, 69, 17, 12, 66, 17, 12, 61, 32, 17, 12, 34, 17, 12, 61, 32, 11, 17, 12, 7131, 16072, 61, 2246, 12, 61, 32, 12095, 7131, 34, 61, 32, 11, 60, 930, 220], [58, 36, 21, 12, 34, 21, 12, 28, 32, 11, 21, 12, 7131, 36, 12, 34, 12, 32, 12095, 7131, 61, 67, 14, 17, 28, 66, 14, 17, 61, 38, 14, 17, 36, 14, 17, 61, 34, 14, 17, 12, 32, 11, 14, 17, 12, 7131, 34, 14, 17, 12, 32, 11, 14, 17, 12, 60, 930, 220], [58, 68, 19, 12, 66, 19, 12, 32, 19, 12, 36, 19, 34, 19, 12, 32, 11, 19, 12, 7131, 68, 18, 66, 18, 32, 18, 36, 18, 12, 34, 18, 12, 32, 11, 18, 12, 7131, 36600, 11, 60, 930, 220], [58, 33, 11, 17, 12, 61, 38, 11, 17, 12, 36, 11, 17, 12, 7131, 33, 12095, 61, 38, 12095, 36, 12095, 7131, 33, 11, 18, 12, 61, 38, 11, 18, 12, 36, 11, 18, 12, 7131, 33, 12095, 61, 38, 12095, 36, 12095, 7131, 61, 67, 14, 17, 61, 32, 14, 17, 28, 38, 14, 17, 33, 11, 14, 17, 12, 61, 38, 11, 14, 17, 12, 36, 11, 14, 17, 7131, 33, 11, 14, 17, 12, 61, 38, 11, 14, 17, 12, 60, 930, 220], [58, 68, 17, 12, 33, 17, 12, 61, 38, 17, 12, 36, 17, 12, 33, 11, 17, 12, 61, 38, 11, 17, 12, 7131, 68, 12, 33, 12, 61, 38, 12, 36, 12, 33, 12095, 61, 38, 12095, 7131, 68, 12, 33, 12, 61, 38, 12, 30195, 12095, 61, 38, 12095, 7131, 68, 17, 12, 33, 17, 12, 61, 38, 17, 12, 36, 17, 12, 33, 11, 17, 12, 61, 38, 11, 17, 12, 7131, 68, 33, 61, 8264, 12, 33, 12095, 61, 38, 12095, 7131, 30195, 11, 61, 38, 11, 60, 930, 220]]\n",
      "/content/drive/MyDrive/test2/9021_128121.abc\n",
      "======total=====\n",
      "[[58, 38, 12, 38, 12095, 7131, 38, 12, 38, 12095, 7131, 38, 12, 38, 12095, 7131, 11190, 11, 7131, 28, 32, 12, 28, 32, 12095, 7131, 28, 32, 12, 28, 32, 12095, 7131, 28, 32, 12, 28, 32, 12095, 7131, 28, 32, 28, 32, 11, 60, 930, 220], [58, 28, 33, 12, 28, 33, 12095, 7131, 28, 33, 12, 28, 33, 12095, 7131, 28, 33, 12, 28, 33, 12095, 7131, 28, 33, 12, 28, 33, 12095, 7131, 28, 33, 12, 28, 33, 12095, 7131, 28, 33, 12, 28, 33, 12095, 7131, 28, 33, 12, 28, 33, 12095, 7131, 28, 33, 14, 17, 28, 33, 11, 14, 17, 60, 87, 14, 17, 930, 220], [58, 68, 12, 66, 62, 14242, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 32, 14, 17, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 12, 66, 8264, 12, 7131, 68, 12, 36, 12, 7131, 68, 12, 66, 8264, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 38, 14, 17, 36, 14, 17, 12, 7131, 68, 14, 17, 36, 14, 17, 60, 930, 220], [58, 68, 12, 66, 62, 33, 15112, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 33, 14, 17, 12, 37, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 33, 14, 17, 37, 14, 17, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 12, 66, 8579, 36, 12, 7131, 68, 12, 36, 12, 7131, 68, 12, 66, 8579, 36, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 37, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 66, 14, 17, 32, 14, 17, 37, 14, 17, 36, 14, 17, 60, 87, 14, 17, 930, 220], [58, 69, 12, 67, 29499, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 67, 14, 17, 12, 33, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 67, 14, 17, 33, 14, 17, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 12, 67, 29499, 12, 7131, 69, 12, 37, 12, 7131, 69, 12, 67, 29499, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 67, 14, 17, 12, 33, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 67, 14, 17, 33, 14, 17, 37, 14, 17, 60, 87, 14, 17, 930, 220], [58, 68, 33, 12, 14242, 33, 12095, 7131, 33, 14, 17, 12, 33, 11, 14, 17, 12, 7131, 68, 14, 17, 12, 33, 14, 17, 12, 32, 14, 17, 12, 36, 14, 17, 12, 33, 11, 14, 17, 12, 7131, 68, 14, 17, 33, 14, 17, 12, 32, 14, 17, 36, 14, 17, 33, 11, 14, 17, 12, 7131, 33, 14, 17, 12, 33, 11, 14, 17, 12, 7131, 68, 33, 12, 8264, 33, 12095, 7131, 33, 12, 33, 12095, 7131, 68, 33, 12, 8264, 33, 12095, 7131, 33, 14, 17, 12, 33, 11, 14, 17, 12, 7131, 68, 14, 17, 12, 33, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 33, 11, 14, 17, 12, 7131, 68, 14, 17, 33, 14, 17, 38, 14, 17, 36, 14, 17, 33, 11, 14, 17, 60, 87, 14, 17, 930, 220], [58, 68, 12, 66, 14242, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 32, 14, 17, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 12, 66, 8264, 12, 7131, 68, 12, 36, 12, 7131, 68, 12, 66, 8264, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 7131, 68, 14, 17, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 930, 220], [58, 721, 4339, 12, 7708, 12095, 7131, 32, 14, 17, 12, 32, 11, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 33, 14, 17, 12, 32, 14, 17, 12, 37, 14, 17, 12, 32, 11, 14, 17, 12, 7131, 68, 14, 17, 66, 14, 17, 33, 14, 17, 32, 14, 17, 12, 37, 14, 17, 32, 11, 14, 17, 12, 7131, 32, 14, 17, 12, 32, 11, 14, 17, 12, 7131, 721, 32, 12, 7708, 12095, 7131, 32, 12, 32, 12095, 7131, 721, 32, 12, 7708, 12095, 7131, 32, 14, 17, 12, 32, 11, 14, 17, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 37, 14, 17, 12, 32, 11, 14, 17, 12, 7131, 68, 14, 17, 66, 14, 17, 32, 14, 17, 37, 14, 17, 32, 11, 14, 17, 60, 87, 14, 17, 930, 220], [58, 21287, 12, 66, 45113, 12, 7131, 67, 14, 17, 12, 35, 14, 17, 12, 7131, 70, 14, 17, 12, 67, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 35, 14, 17, 12, 7131, 70, 14, 17, 67, 14, 17, 12, 66, 14, 17, 38, 14, 17, 35, 14, 17, 12, 7131, 67, 14, 17, 12, 35, 14, 17, 12, 7131, 69, 12, 17896, 12, 28, 32, 12, 37, 12, 35, 7131, 69, 14, 17, 12, 66, 14, 17, 28, 32, 14, 17, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 12, 66, 28, 8579, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 66, 14, 17, 12, 28, 32, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 66, 14, 17, 28, 32, 14, 17, 37, 14, 17, 12, 7131, 69, 14, 17, 37, 14, 17, 60, 930, 220], [58, 70, 12, 67, 28, 40469, 12, 7131, 70, 14, 17, 12, 38, 14, 17, 12, 7131, 70, 14, 17, 12, 67, 14, 17, 12, 28, 33, 14, 17, 12, 38, 14, 17, 12, 7131, 70, 14, 17, 12, 67, 14, 17, 28, 33, 14, 17, 38, 14, 17, 12, 7131, 70, 14, 17, 12, 38, 14, 17, 12, 7131, 70, 12, 68, 12, 28, 33, 12, 38, 12, 7131, 70, 26488, 17, 12, 70, 14, 17, 12, 68, 14, 17, 28, 33, 14, 17, 38, 14, 17, 7131, 70, 26488, 17, 12, 70, 14, 17, 12, 7131, 70, 29001, 70, 12, 68, 28, 40469, 7131, 70, 26488, 17, 12, 70, 14, 17, 12, 7131, 70, 26488, 17, 12, 70, 14, 17, 12, 68, 14, 17, 12, 28, 33, 14, 17, 12, 38, 14, 17, 12, 7131, 70, 26488, 17, 12, 70, 14, 17, 12, 68, 14, 17, 28, 33, 14, 17, 38, 14, 17, 7131, 70, 26488, 17, 70, 14, 17, 60, 930, 220], [58, 70, 12, 16072, 38, 12, 37, 7131, 70, 14, 17, 12, 38, 14, 17, 12, 7131, 70, 14, 17, 12, 69, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 37, 14, 17, 12, 7131, 70, 14, 17, 12, 69, 14, 17, 66, 14, 17, 38, 14, 17, 12, 37, 14, 17, 7131, 70, 14, 17, 12, 38, 14, 17, 12, 7131, 70, 12, 68, 12, 66, 12, 38, 12, 36, 12, 7131, 70, 14, 17, 12, 68, 14, 17, 66, 14, 17, 38, 14, 17, 12, 36, 14, 17, 7131, 70, 14, 17, 12, 38, 14, 17, 12, 7131, 70, 12, 721, 38, 12, 36, 7131, 70, 14, 17, 12, 38, 14, 17, 12, 7131, 70, 14, 17, 12, 68, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 7131, 70, 14, 17, 68, 14, 17, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 930, 220], [58, 69, 12, 17896, 37, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 67, 14, 17, 12, 66, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 67, 14, 17, 66, 14, 17, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 12, 67, 12, 62, 33, 12, 37, 12, 7131, 69, 14, 17, 12, 67, 14, 17, 33, 14, 17, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 12, 67, 29499, 12, 7131, 69, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 12, 67, 14, 17, 12, 33, 14, 17, 12, 37, 14, 17, 12, 7131, 69, 14, 17, 67, 14, 17, 33, 14, 17, 37, 14, 17, 60, 87, 14, 17, 930, 220], [29795, 64, 12, 16072, 32, 12, 7131, 64, 14, 17, 12, 32, 14, 17, 12, 7131, 64, 14, 17, 12, 69, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 7131, 64, 14, 17, 12, 69, 14, 17, 66, 14, 17, 32, 14, 17, 12, 7131, 64, 14, 17, 12, 32, 14, 17, 12, 7131, 64, 12, 69, 12, 66, 12, 32, 12, 7131, 64, 14, 17, 12, 69, 14, 17, 66, 14, 17, 32, 14, 17, 12, 7131, 64, 14, 17, 12, 32, 14, 17, 12, 7131, 64, 12, 16072, 32, 12, 7131, 64, 14, 17, 12, 32, 14, 17, 12, 7131, 64, 14, 17, 12, 69, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 7131, 64, 14, 17, 69, 14, 17, 66, 14, 17, 32, 14, 17, 60, 87, 14, 17, 930, 220], [58, 469, 12, 66, 8264, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 70, 14, 17, 12, 68, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 7131, 70, 14, 17, 68, 14, 17, 12, 66, 14, 17, 38, 14, 17, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 70, 12, 68, 12, 66, 12, 38, 12, 36, 12, 7131, 70, 14, 17, 68, 14, 17, 12, 66, 14, 17, 38, 14, 17, 36, 14, 17, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 469, 12, 66, 8264, 12, 7131, 68, 14, 17, 12, 36, 14, 17, 12, 7131, 70, 14, 17, 12, 68, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 7131, 70, 14, 17, 68, 14, 17, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 930, 220], [58, 21287, 12, 28, 40469, 35, 12, 7131, 67, 14, 17, 12, 35, 14, 17, 12, 7131, 70, 14, 17, 12, 67, 14, 17, 12, 28, 33, 14, 17, 12, 38, 14, 17, 12, 35, 14, 17, 12, 7131, 70, 14, 17, 67, 14, 17, 12, 28, 33, 14, 17, 38, 14, 17, 35, 14, 17, 12, 7131, 67, 14, 17, 12, 35, 14, 17, 12, 7131, 70, 12, 67, 12, 28, 33, 12, 38, 12, 35, 12, 7131, 70, 14, 17, 67, 14, 17, 12, 28, 33, 14, 17, 38, 14, 17, 35, 14, 17, 12, 7131, 67, 14, 17, 12, 35, 14, 17, 12, 7131, 21287, 12, 28, 40469, 35, 12, 7131, 67, 14, 17, 12, 35, 14, 17, 12, 7131, 70, 14, 17, 12, 67, 14, 17, 12, 28, 33, 14, 17, 12, 38, 14, 17, 12, 35, 14, 17, 12, 7131, 70, 14, 17, 67, 14, 17, 28, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 14, 17, 930, 220], [58, 469, 66, 36, 60, 87, 14, 17, 58, 70, 14, 17, 12, 68, 14, 17, 12, 66, 14, 17, 12, 36, 14, 17, 12, 7131, 70, 14, 17, 68, 14, 17, 66, 14, 17, 36, 14, 17, 60, 87, 14, 17, 58, 469, 66, 36, 60, 87, 58, 64, 721, 36, 60, 87, 14, 17, 29795, 65, 14, 17, 12, 68, 14, 17, 12, 66, 14, 17, 12, 36, 14, 17, 12, 7131, 65, 14, 17, 68, 14, 17, 66, 14, 17, 36, 14, 17, 60, 87, 14, 17, 930, 220]]\n",
      "/content/drive/MyDrive/test2/8579_7977.abc\n",
      "======total=====\n",
      "[[58, 36, 12, 34, 12, 32, 11, 32, 9832, 12, 7131, 36, 14, 17, 34, 14, 17, 36, 11, 14, 17, 32, 9832, 14, 17, 12, 60, 32, 9832, 14, 17, 49146, 32, 11, 14, 17, 32, 9832, 14, 17, 12, 60, 32, 9832, 14, 17, 49146, 36, 14, 17, 12, 34, 14, 17, 12, 32, 11, 14, 17, 32, 9832, 14, 17, 12, 7131, 36, 14, 17, 34, 14, 17, 32, 9832, 14, 17, 12, 7131, 36, 11, 32, 9832, 12, 7131, 32, 11, 32, 9832, 60, 34, 14, 17, 87, 14, 17, 36, 930, 220], [34, 12095, 58, 38, 11, 34, 11, 60, 34, 14, 17, 87, 14, 17, 58, 38, 14, 17, 36, 14, 17, 60, 87, 14, 17, 58, 37, 14, 17, 12, 35, 14, 17, 32, 11, 14, 17, 12, 35, 11, 14, 17, 12, 7131, 37, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 12, 7131, 35, 12, 35, 12095, 7131, 35, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 60, 87, 14, 17, 32, 14, 17, 87, 14, 17, 930, 220], [58, 35, 17, 12, 33, 11, 17, 12, 38, 11, 17, 38, 9832, 17, 12, 7131, 35, 14, 17, 12, 33, 11, 14, 17, 35, 11, 14, 17, 38, 9832, 14, 17, 12, 7131, 35, 14, 17, 38, 9832, 14, 17, 12, 7131, 38, 11, 38, 9832, 12, 7131, 35, 11, 38, 9832, 12, 7131, 35, 12, 32, 12095, 38, 9832, 12, 7131, 35, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 12, 38, 9832, 14, 17, 60, 35, 11, 14, 17, 38, 11, 14, 17, 38, 9832, 14, 17, 12, 930, 220], [58, 35, 17, 12, 33, 11, 17, 12, 38, 11, 17, 38, 9832, 17, 12, 7131, 35, 14, 17, 12, 33, 11, 14, 17, 35, 11, 14, 17, 12, 38, 9832, 14, 17, 12, 7131, 35, 14, 17, 12, 35, 11, 14, 17, 38, 9832, 14, 17, 12, 7131, 35, 14, 17, 38, 11, 14, 17, 38, 9832, 14, 17, 12, 60, 38, 9832, 14, 17, 58, 35, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 60, 87, 14, 17, 38, 9832, 49146, 35, 11, 14, 17, 38, 9832, 14, 17, 60, 87, 14, 17, 58, 35, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 60, 87, 14, 17, 930, 220], [58, 36, 12, 34, 12, 32, 12095, 32, 9832, 12, 7131, 36, 14, 17, 12, 34, 14, 17, 12, 32, 11, 14, 17, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 12, 7131, 36, 14, 17, 12, 34, 14, 17, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 12, 7131, 36, 14, 17, 32, 11, 14, 17, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 12, 7131, 36, 11, 14, 17, 32, 9832, 14, 17, 12, 7131, 36, 12, 32, 9832, 12, 7131, 6500, 11, 32, 9832, 12, 7131, 34, 12, 32, 9832, 12, 7131, 34, 14, 17, 12, 36, 11, 14, 17, 32, 9832, 14, 17, 60, 34, 14, 17, 32, 11, 14, 17, 87, 14, 17, 930, 220], [34, 12095, 58, 36, 12095, 34, 12095, 7131, 38, 11, 36, 11, 34, 11, 60, 34, 14, 17, 87, 14, 17, 35, 12095, 58, 37, 12095, 35, 12095, 7131, 32, 11, 14, 17, 37, 11, 14, 17, 12, 35, 11, 14, 17, 12, 7131, 37, 11, 14, 17, 35, 11, 14, 17, 60, 35, 14, 17, 87, 14, 17, 930, 220], [38, 9832, 49146, 35, 12095, 38, 9832, 12, 7131, 38, 11, 14, 17, 35, 11, 14, 17, 38, 9832, 14, 17, 12, 60, 38, 9832, 14, 17, 49146, 35, 12, 32, 12095, 38, 9832, 12, 7131, 35, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 12, 38, 9832, 14, 17, 12, 7131, 35, 11, 14, 17, 38, 9832, 14, 17, 12, 7131, 35, 14, 17, 12, 33, 11, 14, 17, 12, 38, 9832, 14, 17, 7131, 35, 14, 17, 12, 33, 11, 14, 17, 60, 35, 14, 17, 87, 14, 17, 58, 35, 12, 34, 12, 38, 12095, 60, 930, 220], [58, 35, 14, 17, 12, 34, 14, 17, 12, 38, 11, 14, 17, 38, 9832, 14, 17, 12, 7131, 35, 14, 17, 34, 14, 17, 38, 9832, 14, 17, 12, 7131, 35, 12, 33, 12095, 38, 9832, 12, 7131, 35, 14, 17, 33, 11, 14, 17, 35, 11, 14, 17, 12, 38, 9832, 14, 17, 12, 7131, 35, 11, 14, 17, 38, 9832, 14, 17, 12, 7131, 35, 12, 38, 12095, 38, 9832, 12, 7131, 35, 14, 17, 12, 38, 11, 14, 17, 35, 11, 14, 17, 12, 38, 9832, 14, 17, 12, 7131, 35, 14, 17, 12, 35, 11, 14, 17, 38, 9832, 14, 17, 12, 7131, 35, 14, 17, 12, 32, 11, 14, 17, 12, 38, 9832, 14, 17, 7131, 35, 14, 17, 12, 32, 11, 14, 17, 7131, 35, 14, 17, 38, 11, 14, 17, 60, 87, 18, 14, 17, 930, 220], [58, 33, 11, 18, 14, 17, 12, 38, 11, 18, 14, 17, 36, 11, 18, 14, 17, 36, 9832, 18, 14, 17, 12, 7131, 33, 11, 14, 17, 36, 9832, 14, 17, 12, 7131, 33, 9832, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 49146, 36, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 49146, 33, 9832, 36, 9832, 12, 7131, 37, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 49146, 33, 9832, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 49146, 38, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 12, 930, 220], [58, 33, 9832, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 49146, 36, 14, 17, 36, 9832, 14, 17, 60, 87, 14, 17, 36, 9832, 49146, 33, 11, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 49146, 33, 9832, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 49146, 36, 11, 36, 9832, 60, 38, 11, 33, 11, 930, 220], [34, 12095, 58, 36, 12095, 34, 12095, 7131, 38, 11, 36, 12095, 34, 12095, 7131, 38, 12, 35, 12, 36, 12095, 34, 12095, 7131, 38, 12, 35, 38, 12095, 36, 12095, 34, 12095, 7131, 38, 14, 17, 12, 34, 14, 17, 12, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 14, 17, 12, 34, 14, 17, 12, 7131, 38, 12, 34, 12, 34, 12095, 7131, 38, 14, 17, 12, 34, 14, 17, 38, 11, 14, 17, 12, 34, 11, 14, 17, 12, 7131, 38, 14, 17, 12, 38, 11, 14, 17, 12, 34, 11, 14, 17, 12, 60, 930, 220], [58, 38, 12, 34, 12, 38, 12095, 34, 12095, 7131, 38, 14, 17, 12, 34, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 12, 34, 11, 14, 17, 12, 7131, 38, 14, 17, 36, 11, 14, 17, 12, 34, 11, 14, 17, 12, 7131, 38, 11, 36, 12095, 34, 12095, 7131, 38, 12, 35, 12, 36, 12095, 34, 12095, 7131, 38, 12, 35, 38, 11, 36, 12095, 34, 12095, 7131, 38, 12, 34, 12, 36, 12095, 34, 12095, 7131, 38, 14, 17, 34, 14, 17, 38, 11, 14, 17, 12, 36, 11, 14, 17, 34, 11, 14, 17, 60, 38, 11, 14, 17, 34, 11, 930, 220], [58, 36, 17, 12, 34, 17, 12, 38, 11, 17, 32, 9832, 17, 12, 7131, 2943, 36, 12095, 32, 9832, 12, 7131, 38, 11, 14, 17, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 12, 7131, 36, 11, 32, 9832, 12, 60, 32, 9832, 14, 17, 49146, 36, 14, 17, 12, 34, 14, 17, 12, 32, 9832, 14, 17, 7131, 36, 14, 17, 12, 34, 14, 17, 12, 7131, 36, 14, 17, 34, 14, 17, 32, 9832, 14, 17, 12, 60, 32, 9832, 14, 17, 49146, 32, 12095, 32, 9832, 12, 60, 930, 220], [58, 32, 11, 14, 17, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 12, 7131, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 12, 7131, 38, 11, 14, 17, 12, 36, 11, 14, 17, 32, 9832, 14, 17, 60, 38, 11, 14, 17, 49146, 38, 11, 14, 17, 32, 9832, 14, 17, 12, 60, 32, 9832, 14, 17, 49146, 34, 12, 32, 9832, 12, 7131, 5222, 11, 32, 9832, 12, 7131, 32, 11, 14, 17, 32, 9832, 14, 17, 60, 87, 14, 17, 34, 14, 17, 87, 14, 17, 36, 14, 17, 87, 14, 17, 930, 220], [87, 35, 12095, 58, 35, 14, 17, 35, 11, 14, 17, 12, 60, 35, 11, 14, 17, 49146, 37, 14, 17, 35, 14, 17, 35, 11, 14, 17, 12, 60, 35, 11, 14, 17, 49146, 37, 14, 17, 35, 14, 17, 35, 11, 14, 17, 60, 87, 14, 17, 58, 37, 14, 17, 35, 14, 17, 60, 87, 14, 17, 58, 38, 14, 17, 36, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 60, 87, 14, 17, 58, 32, 11, 14, 17, 35, 11, 14, 17, 60, 87, 14, 17, 930, 220], [58, 32, 14, 17, 37, 14, 17, 35, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 60, 87, 21, 87, 18, 14, 17, 930, 220]]\n",
      "/content/drive/MyDrive/test2/8750_118784.abc\n",
      "======total=====\n",
      "[[58, 68, 21, 12, 33, 21, 12, 38, 21, 12, 36, 21, 12, 33, 11, 21, 12, 38, 11, 21, 12, 36, 11, 21, 12, 33, 9832, 21, 12, 36, 9832, 21, 12, 7131, 68, 18, 14, 17, 33, 18, 14, 17, 12, 38, 18, 14, 17, 12, 36, 18, 14, 17, 12, 33, 11, 18, 14, 17, 12, 38, 11, 18, 14, 17, 12, 36, 11, 18, 14, 17, 12, 33, 9832, 18, 14, 17, 12, 36, 9832, 18, 14, 17, 7131, 33, 14, 17, 38, 14, 17, 36, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 33, 9832, 14, 17, 60, 930, 220], [58, 69, 17, 28, 66, 17, 32, 17, 37, 17, 35, 17, 12, 34, 17, 32, 11, 17, 35, 11, 17, 12, 7131, 69, 14, 17, 66, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 12, 34, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 12, 7131, 35, 14, 17, 12, 35, 11, 14, 17, 12, 7131, 721, 32, 1961, 12, 8141, 11, 35, 12095, 7131, 721, 1961, 12, 8610, 12095, 7131, 16072, 8579, 35, 8141, 11, 35, 11, 7131, 69, 14, 17, 66, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 34, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 60, 87, 14, 17, 58, 70, 12, 66, 4760, 12, 8141, 11, 60, 930, 220], [58, 70, 14, 17, 66, 14, 17, 12, 32, 14, 17, 12, 38, 14, 17, 35, 14, 17, 12, 34, 14, 17, 12, 32, 11, 14, 17, 12, 35, 11, 14, 17, 12, 7131, 66, 14, 17, 32, 14, 17, 12, 35, 14, 17, 12, 34, 14, 17, 32, 11, 14, 17, 12, 35, 11, 14, 17, 12, 7131, 16072, 8579, 35, 8141, 11, 35, 11, 7131, 16072, 32, 12, 26009, 12, 8141, 12095, 35, 12095, 7131, 68, 12, 66, 12, 14242, 12, 35, 12, 34, 12, 32, 11, 35, 12095, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 36, 14, 17, 12, 35, 14, 17, 12, 34, 14, 17, 12, 32, 11, 14, 17, 35, 11, 14, 17, 12, 7131, 68, 14, 17, 66, 14, 17, 36, 14, 17, 35, 14, 17, 12, 34, 14, 17, 35, 11, 14, 17, 12, 7131, 16072, 32, 12, 26009, 12, 8141, 12095, 35, 12095, 7131, 69, 14, 17, 66, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 12, 34, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 12, 7131, 35, 14, 17, 12, 35, 11, 14, 17, 12, 7131, 69, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 34, 14, 17, 12, 32, 11, 14, 17, 12, 35, 11, 14, 17, 7131, 69, 14, 17, 66, 14, 17, 32, 14, 17, 37, 14, 17, 34, 14, 17, 32, 11, 14, 17, 60, 930, 220], [58, 70, 12, 67, 38, 12, 35, 12, 33, 12095, 38, 12095, 35, 12095, 33, 9832, 12, 38, 9832, 12, 7131, 70, 38, 12, 11012, 11, 38, 12095, 35, 11, 33, 9832, 38, 9832, 12, 7131, 21287, 38, 12, 35, 12, 33, 12095, 38, 12095, 35, 12095, 33, 9832, 12, 38, 9832, 12, 7131, 70, 14, 17, 12, 67, 14, 17, 12, 38, 14, 17, 12, 35, 14, 17, 12, 33, 11, 14, 17, 38, 11, 14, 17, 12, 35, 11, 14, 17, 12, 33, 9832, 14, 17, 38, 9832, 14, 17, 12, 7131, 70, 14, 17, 67, 14, 17, 38, 14, 17, 12, 35, 14, 17, 38, 11, 14, 17, 35, 11, 14, 17, 38, 9832, 14, 17, 7131, 21287, 45113, 38, 11, 35, 11, 7131, 21287, 38, 12, 35, 38, 11, 7131, 21287, 38, 11012, 11, 38, 11, 33, 9832, 7131, 21287, 45113, 38, 11, 60, 930, 220], [58, 721, 14242, 12, 8141, 11, 36, 12095, 32, 9832, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 36, 14, 17, 12, 34, 14, 17, 12, 32, 11, 14, 17, 12, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 7131, 68, 14, 17, 66, 14, 17, 32, 14, 17, 36, 14, 17, 12, 34, 14, 17, 32, 11, 14, 17, 36, 11, 14, 17, 7131, 68, 14, 17, 66, 14, 17, 32, 14, 17, 36, 14, 17, 34, 14, 17, 32, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 68, 17, 66, 17, 32, 17, 36, 17, 34, 17, 32, 11, 17, 7131, 721, 14242, 12, 8141, 11, 7131, 68, 14, 17, 66, 14, 17, 32, 14, 17, 36, 14, 17, 34, 14, 17, 32, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 68, 14, 17, 66, 14, 17, 32, 14, 17, 36, 14, 17, 34, 14, 17, 32, 11, 14, 17, 60, 87, 14, 17, 930, 220], [58, 36484, 38, 12, 36, 12, 34, 12, 38, 11, 36, 12095, 34, 12095, 7131, 36484, 38, 12, 2943, 38, 11, 36, 11, 34, 11, 7131, 70, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 34, 14, 17, 12, 36, 11, 14, 17, 12, 34, 11, 14, 17, 7131, 70, 14, 17, 66, 14, 17, 38, 14, 17, 36, 14, 17, 12, 34, 14, 17, 36, 11, 14, 17, 12, 7131, 70, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 34, 14, 17, 12, 38, 11, 14, 17, 12, 36, 11, 14, 17, 7131, 70, 14, 17, 66, 14, 17, 38, 14, 17, 12, 34, 14, 17, 38, 11, 14, 17, 7131, 70, 17, 66, 17, 38, 17, 12, 34, 17, 38, 11, 17, 34, 11, 17, 7131, 36484, 15916, 12, 38, 11, 34, 12095, 7131, 36484, 15916, 38, 11, 34, 11, 60, 930, 220], [58, 721, 32, 36600, 11, 36, 12095, 32, 9832, 12, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 32, 14, 17, 12, 36, 14, 17, 12, 34, 14, 17, 12, 32, 11, 14, 17, 12, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 7131, 68, 14, 17, 66, 14, 17, 32, 14, 17, 36, 14, 17, 12, 34, 14, 17, 32, 11, 14, 17, 36, 11, 14, 17, 7131, 68, 14, 17, 66, 14, 17, 32, 14, 17, 36, 14, 17, 34, 14, 17, 32, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 68, 18, 14, 17, 66, 18, 14, 17, 12, 32, 18, 14, 17, 12, 36, 18, 14, 17, 12, 34, 18, 14, 17, 12, 32, 11, 18, 14, 17, 12, 36, 11, 18, 14, 17, 12, 7131, 66, 14, 17, 32, 14, 17, 36, 14, 17, 34, 14, 17, 32, 11, 14, 17, 36, 11, 14, 17, 7131, 721, 14242, 12, 8141, 11, 7131, 68, 14, 17, 66, 14, 17, 32, 14, 17, 36, 14, 17, 34, 14, 17, 32, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 721, 32, 36600, 11, 60, 930, 220], [58, 68, 12, 40469, 12, 37, 12, 36, 12, 33, 12095, 38, 12095, 37, 12095, 36, 12095, 33, 9832, 12, 36, 9832, 12, 7131, 68, 38, 15112, 33, 11, 38, 11, 37, 11, 36, 12095, 33, 9832, 36, 9832, 12, 7131, 68, 33, 21713, 12, 30195, 12095, 38, 11, 37, 12095, 36, 12095, 33, 9832, 12, 36, 9832, 12, 7131, 68, 14, 17, 12, 33, 14, 17, 12, 38, 14, 17, 12, 37, 14, 17, 12, 36, 14, 17, 12, 33, 11, 14, 17, 12, 38, 11, 14, 17, 12, 37, 11, 14, 17, 12, 36, 11, 14, 17, 12, 33, 9832, 14, 17, 36, 9832, 14, 17, 12, 7131, 68, 14, 17, 33, 14, 17, 38, 14, 17, 37, 14, 17, 36, 14, 17, 12, 33, 11, 14, 17, 38, 11, 14, 17, 37, 11, 14, 17, 36, 11, 14, 17, 36, 9832, 14, 17, 7131, 68, 33, 8264, 33, 11, 38, 11, 36, 11, 7131, 68, 33, 8264, 33, 11, 38, 11, 7131, 68, 33, 8264, 33, 12095, 38, 11, 33, 9832, 12, 7131, 68, 33, 8264, 33, 11, 38, 11, 33, 9832, 60, 930, 220], [58, 16344, 8579, 35, 12, 32, 11, 35, 12095, 7131, 69, 14, 17, 12, 67, 14, 17, 12, 32, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 12, 32, 11, 14, 17, 12, 35, 11, 14, 17, 7131, 69, 14, 17, 67, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 32, 11, 14, 17, 7131, 69, 14, 17, 67, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 32, 11, 14, 17, 60, 87, 14, 17, 58, 16344, 8579, 5631, 11, 7131, 16344, 8579, 5631, 11, 7131, 16344, 8579, 5631, 11, 7131, 69, 14, 17, 67, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 32, 11, 14, 17, 60, 87, 14, 17, 58, 69, 14, 17, 67, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 32, 11, 14, 17, 60, 87, 14, 17, 930, 220], [58, 36484, 4760, 8141, 11, 35, 12095, 7131, 69, 12, 66, 8579, 12, 8141, 11, 35, 11, 7131, 69, 14, 17, 12, 66, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 34, 14, 17, 12, 35, 11, 14, 17, 7131, 69, 14, 17, 66, 14, 17, 37, 14, 17, 34, 14, 17, 7131, 68, 18, 14, 17, 66, 18, 14, 17, 12, 32, 18, 14, 17, 12, 36, 18, 14, 17, 34, 18, 14, 17, 12, 32, 11, 18, 14, 17, 12, 7131, 66, 14, 17, 32, 14, 17, 34, 14, 17, 32, 11, 14, 17, 7131, 16344, 8579, 5631, 11, 7131, 69, 14, 17, 66, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 34, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 60, 87, 14, 17, 58, 16344, 8579, 35, 12, 32, 11, 60, 930, 220], [58, 21287, 38, 12, 35, 12, 38, 12095, 35, 12095, 38, 9832, 12, 7131, 21287, 38, 12, 35, 38, 12095, 35, 11, 38, 9832, 12, 7131, 70, 14, 17, 67, 14, 17, 38, 14, 17, 36, 14, 17, 35, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 38, 9832, 14, 17, 12, 60, 38, 9832, 14, 17, 49146, 69, 14, 17, 12, 67, 14, 17, 12, 32, 14, 17, 12, 37, 14, 17, 12, 35, 14, 17, 12, 32, 11, 14, 17, 12, 35, 11, 14, 17, 12, 38, 9832, 14, 17, 7131, 16344, 12, 32, 12, 26009, 12, 32, 12095, 35, 12095, 7131, 67, 14, 17, 32, 14, 17, 35, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 7131, 16344, 8579, 5631, 11, 7131, 69, 14, 17, 67, 14, 17, 32, 14, 17, 37, 14, 17, 35, 14, 17, 32, 11, 14, 17, 35, 11, 14, 17, 60, 87, 14, 17, 58, 16344, 8579, 5631, 11, 60, 930, 220], [58, 68, 33, 8264, 12, 33, 11, 38, 11, 36, 12095, 36, 9832, 12, 7131, 68, 14, 17, 12, 33, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 33, 11, 14, 17, 12, 38, 11, 14, 17, 12, 36, 11, 14, 17, 12, 36, 9832, 14, 17, 7131, 68, 14, 17, 33, 14, 17, 38, 14, 17, 36, 14, 17, 12, 33, 11, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 12, 7131, 68, 14, 17, 33, 14, 17, 38, 14, 17, 36, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 68, 18, 14, 17, 33, 18, 14, 17, 12, 38, 18, 14, 17, 12, 36, 18, 14, 17, 12, 33, 11, 18, 14, 17, 12, 38, 11, 18, 14, 17, 12, 36, 11, 18, 14, 17, 12, 7131, 33, 14, 17, 38, 14, 17, 36, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 7131, 68, 33, 8264, 12, 33, 11, 38, 11, 7131, 68, 14, 17, 33, 14, 17, 38, 14, 17, 36, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 68, 33, 8264, 33, 11, 38, 11, 60, 930, 220], [58, 68, 12, 66, 32, 12, 36, 12, 8141, 12095, 36, 12095, 32, 9832, 12, 7131, 68, 14242, 12, 32, 12095, 36, 11, 32, 9832, 12, 7131, 721, 14242, 12, 8141, 11, 36, 12095, 32, 9832, 12, 7131, 721, 14242, 12, 8141, 11, 36, 11, 32, 9832, 7131, 721, 14242, 12, 8141, 11, 36, 12095, 7131, 721, 32, 36600, 11, 36, 11, 7131, 21287, 38, 11012, 12095, 38, 11, 33, 9832, 12, 7131, 21287, 38, 11012, 11, 38, 11, 33, 9832, 60, 930, 220], [58, 68, 17, 67, 17, 36, 17, 35, 17, 34, 17, 38, 11, 17, 34, 11, 17, 7131, 68, 33, 12, 8264, 33, 12095, 38, 11, 7131, 68, 33, 8264, 33, 11, 38, 11, 7131, 68, 11879, 32, 11, 38, 11, 7131, 68, 14, 17, 12, 66, 14, 17, 12, 38, 14, 17, 36, 14, 17, 12, 34, 14, 17, 12, 38, 11, 14, 17, 7131, 68, 14, 17, 66, 14, 17, 36, 14, 17, 34, 14, 17, 7131, 68, 14, 17, 66, 14, 17, 38, 14, 17, 36, 14, 17, 34, 14, 17, 38, 11, 14, 17, 60, 87, 14, 17, 58, 721, 38, 2943, 38, 11, 60, 930, 220], [58, 68, 33, 8264, 12, 33, 11, 38, 11, 36, 12095, 36, 9832, 12, 7131, 68, 33, 8264, 12, 33, 11, 38, 11, 36, 11, 36, 9832, 12, 7131, 68, 14, 17, 33, 14, 17, 38, 14, 17, 36, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 36, 9832, 14, 17, 12, 60, 36, 9832, 14, 17, 49146, 68, 14, 17, 12, 33, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 33, 11, 14, 17, 12, 38, 11, 14, 17, 12, 36, 11, 14, 17, 12, 36, 9832, 14, 17, 7131, 68, 18, 14, 17, 33, 18, 14, 17, 38, 18, 14, 17, 36, 18, 14, 17, 12, 33, 11, 18, 14, 17, 38, 11, 18, 14, 17, 36, 11, 18, 14, 17, 7131, 68, 33, 8264, 12, 33, 11, 38, 11, 7131, 68, 14, 17, 33, 14, 17, 38, 14, 17, 36, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 68, 33, 8264, 33, 11, 38, 11, 60, 930, 220], [58, 68, 61, 66, 8264, 12, 61, 8141, 12095, 38, 11, 36, 12095, 32, 9832, 12, 7131, 68, 14, 17, 12, 61, 66, 14, 17, 12, 38, 14, 17, 12, 36, 14, 17, 12, 61, 34, 14, 17, 12, 32, 11, 14, 17, 38, 11, 14, 17, 12, 36, 11, 14, 17, 12, 32, 9832, 14, 17, 7131, 68, 14, 17, 61, 66, 14, 17, 38, 14, 17, 36, 14, 17, 12, 61, 34, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 7131, 68, 14, 17, 61, 66, 14, 17, 38, 14, 17, 36, 14, 17, 61, 34, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 68, 17, 61, 66, 17, 38, 17, 36, 17, 61, 34, 17, 38, 11, 17, 36, 11, 17, 7131, 68, 61, 66, 8264, 12, 61, 39816, 11, 7131, 68, 14, 17, 61, 66, 14, 17, 38, 14, 17, 36, 14, 17, 61, 34, 14, 17, 38, 11, 14, 17, 36, 11, 14, 17, 60, 87, 14, 17, 58, 68, 61, 66, 8264, 61, 39816, 11, 60, 930, 220]]\n",
      "/content/drive/MyDrive/test2/8269_150924.abc\n",
      "======total=====\n",
      "[[58, 32, 14, 17, 32, 9832, 14, 17, 12, 7131, 38, 14, 17, 32, 9832, 14, 17, 7131, 37, 14, 17, 32, 11, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 38, 14, 17, 32, 11, 14, 17, 37, 11, 14, 17, 7131, 32, 12, 35, 11, 7131, 32, 12, 32, 11, 37, 11, 7131, 3838, 9832, 7131, 32, 7708, 11, 37, 11, 7131, 33, 11190, 9832, 7131, 33, 4579, 11, 38, 11, 60, 930, 220], [58, 67, 12, 33, 12, 35, 11, 7131, 67, 14, 17, 33, 14, 17, 33, 11, 14, 17, 12, 38, 11, 14, 17, 12, 7131, 33, 14, 17, 38, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 7131, 2885, 11, 7131, 3838, 11, 37, 11, 7131, 32, 14, 17, 32, 9832, 14, 17, 12, 7131, 38, 14, 17, 32, 9832, 14, 17, 7131, 7708, 11, 37, 11, 7131, 38, 61, 32, 9832, 7131, 21713, 11, 36, 11, 60, 930, 220], [58, 22495, 9832, 7131, 21713, 11, 36, 11, 7131, 26001, 9832, 7131, 5777, 11, 35, 11, 7131, 37, 14, 17, 33, 9832, 14, 17, 12, 7131, 36, 14, 17, 33, 9832, 14, 17, 7131, 8068, 11, 35, 11, 7131, 40469, 28, 32, 9832, 7131, 33, 4579, 11, 38, 11, 60, 930, 220], [58, 67, 12, 33, 12, 35, 11, 7131, 67, 14, 17, 33, 14, 17, 33, 11, 14, 17, 12, 38, 11, 14, 17, 12, 7131, 33, 14, 17, 38, 14, 17, 33, 11, 14, 17, 38, 11, 14, 17, 7131, 8579, 35, 11, 7131, 32, 7708, 11, 37, 11, 7131, 32, 14, 17, 32, 9832, 14, 17, 12, 7131, 38, 14, 17, 32, 9832, 14, 17, 7131, 7708, 11, 37, 11, 7131, 38, 2943, 11, 7131, 8264, 32, 11, 38, 11, 60, 930, 220], [58, 8141, 9832, 7131, 36600, 11, 38, 11, 7131, 5631, 11, 37, 11, 35, 11, 7131, 5631, 11, 37, 11, 35, 11, 7131, 5631, 11, 37, 11, 35, 11, 60, 37, 14, 17, 37, 14, 17, 37, 49146, 37, 12, 34, 61, 32, 11, 37, 11, 37, 9832, 60, 930, 220], [58, 37, 12, 11012, 11, 37, 11, 37, 9832, 7131, 37, 12, 2943, 37, 11, 37, 9832, 7131, 37, 11012, 11, 33, 9832, 33, 9832, 11, 60, 35, 14, 17, 34, 14, 17, 35, 14, 17, 36, 14, 17, 37, 14, 17, 38, 14, 17, 28, 32, 49146, 32, 12, 36600, 11, 32, 9832, 60, 930, 220], [58, 32, 12, 37, 5631, 11, 32, 9832, 7131, 32, 12, 8264, 32, 11, 32, 9832, 7131, 8579, 16458, 11, 35, 9832, 60, 37, 14, 17, 28, 37, 14, 17, 61, 37, 14, 17, 61, 38, 14, 17, 32, 14, 17, 33, 14, 17, 66, 49146, 66, 12, 61, 38, 28, 37, 4093, 11, 60, 930, 220], [58, 66, 12, 32, 61, 37, 4093, 11, 7131, 66, 12, 33, 61, 38, 4093, 11, 7131, 66, 32, 5777, 11, 37, 9832, 60, 32, 14, 17, 61, 38, 14, 17, 32, 14, 17, 33, 14, 17, 66, 14, 17, 67, 14, 17, 68, 49146, 68, 12, 33, 61, 38, 6500, 11, 60, 930, 220], [58, 68, 12, 66, 32, 6500, 11, 7131, 68, 12, 36077, 6500, 11, 7131, 721, 3838, 11, 32, 9832, 60, 87, 17, 7, 18, 32, 14, 17, 33, 14, 17, 66, 14, 17, 58, 67, 35, 11, 7131, 67, 26853, 11, 37, 11, 60, 930, 220], [58, 67, 14, 17, 32, 14, 17, 32, 9832, 14, 17, 12, 7131, 66, 14, 17, 32, 9832, 14, 17, 7131, 67, 14, 17, 35, 14, 17, 12, 32, 11, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 68, 14, 17, 35, 14, 17, 32, 11, 14, 17, 37, 11, 14, 17, 7131, 69, 35, 11, 7131, 16344, 5631, 11, 37, 11, 7131, 69, 14, 17, 67, 14, 17, 32, 9832, 14, 17, 12, 7131, 28, 69, 14, 17, 32, 9832, 14, 17, 7131, 61, 69, 14, 17, 35, 14, 17, 12, 32, 11, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 28, 70, 14, 17, 35, 14, 17, 32, 11, 14, 17, 37, 11, 14, 17, 7131, 64, 35, 11, 7131, 1878, 5631, 11, 37, 11, 60, 930, 220], [58, 64, 14, 17, 69, 14, 17, 32, 9832, 14, 17, 12, 7131, 61, 70, 14, 17, 32, 9832, 14, 17, 7131, 64, 14, 17, 35, 14, 17, 12, 32, 11, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 67, 26488, 17, 35, 14, 17, 32, 11, 14, 17, 37, 11, 14, 17, 7131, 64, 12, 69, 12, 35, 11, 7131, 64, 12, 69, 12, 5631, 11, 37, 11, 7131, 1878, 32, 9832, 7131, 69, 26853, 11, 37, 11, 7131, 28, 70, 14242, 11, 7131, 70, 14, 17, 32, 14, 17, 12, 34, 14, 17, 12, 32, 11, 14, 17, 12, 38, 11, 14, 17, 12, 7131, 69, 14, 17, 32, 14, 17, 34, 14, 17, 32, 11, 14, 17, 38, 11, 14, 17, 60, 930, 220], [58, 68, 3838, 9832, 7131, 70, 26576, 11, 38, 11, 7131, 69, 2885, 11, 7131, 69, 14, 17, 32, 14, 17, 12, 35, 14, 17, 12, 32, 11, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 68, 14, 17, 32, 14, 17, 35, 14, 17, 32, 11, 14, 17, 37, 11, 14, 17, 7131, 67, 3838, 9832, 7131, 69, 26853, 11, 37, 11, 7131, 68, 61, 8264, 11, 7131, 33, 61, 38, 11012, 11, 61, 38, 11, 60, 930, 220], [58, 66, 61, 8264, 11, 7131, 67, 61, 38, 11012, 11, 61, 38, 11, 7131, 68, 12, 66, 12, 32, 12, 28, 38, 12, 7131, 68, 12, 66, 12, 32, 12, 38, 12, 32, 9832, 32, 9832, 11, 7131, 721, 32, 4579, 9832, 33, 9832, 11, 7131, 32, 14, 17, 34, 11, 14, 17, 12, 34, 9832, 14, 17, 12, 7131, 66, 14, 17, 33, 14, 17, 34, 11, 14, 17, 34, 9832, 14, 17, 7131, 67, 35, 11, 35, 9832, 7131, 67, 26853, 11, 37, 11, 60, 930, 220], [58, 67, 14, 17, 32, 14, 17, 32, 9832, 14, 17, 12, 7131, 66, 14, 17, 32, 9832, 14, 17, 7131, 67, 14, 17, 35, 14, 17, 12, 32, 11, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 68, 14, 17, 35, 14, 17, 32, 11, 14, 17, 37, 11, 14, 17, 7131, 69, 35, 11, 7131, 16344, 5631, 11, 37, 11, 7131, 69, 14, 17, 67, 14, 17, 32, 9832, 14, 17, 12, 7131, 28, 69, 14, 17, 32, 9832, 14, 17, 7131, 61, 69, 14, 17, 35, 14, 17, 12, 32, 11, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 70, 14, 17, 35, 14, 17, 32, 11, 14, 17, 37, 11, 14, 17, 7131, 64, 35, 11, 7131, 1878, 5631, 11, 37, 11, 60, 930, 220], [58, 64, 14, 17, 69, 14, 17, 32, 9832, 14, 17, 12, 7131, 61, 70, 14, 17, 32, 9832, 14, 17, 7131, 64, 14, 17, 35, 14, 17, 12, 32, 11, 14, 17, 12, 37, 11, 14, 17, 12, 7131, 67, 26488, 17, 35, 14, 17, 32, 11, 14, 17, 37, 11, 14, 17, 7131, 64, 12, 69, 12, 35, 11, 7131, 64, 12, 69, 12, 5631, 11, 37, 11, 7131, 1878, 5631, 11, 37, 11, 60, 67, 58, 17457, 33, 28, 11190, 11, 7131, 324, 32, 5777, 11, 60, 930, 220], [58, 36484, 32, 6500, 11, 7131, 16344, 29266, 11, 7131, 68, 32, 2943, 34, 11, 7131, 67, 61, 8264, 33, 11, 33, 9832, 7131, 66, 14242, 32, 11, 32, 9832, 7131, 67, 2885, 37, 11, 37, 9832, 7131, 68, 12473, 28, 38, 12095, 38, 9832, 12, 7131, 69, 14, 17, 38, 11, 14, 17, 12, 38, 9832, 14, 17, 12, 7131, 70, 14, 17, 38, 11, 14, 17, 38, 9832, 14, 17, 60, 930, 220]]\n",
      "/content/drive/MyDrive/test2/9184_127212.abc\n",
      "======total=====\n",
      "[[58, 11012, 11, 38, 11, 7131, 11012, 11, 38, 11, 7131, 11012, 11, 38, 11, 7131, 35, 17, 33, 11, 17, 38, 11, 17, 7131, 11012, 11, 38, 11, 7131, 11012, 11, 38, 11, 7131, 11012, 11, 38, 11, 60, 930, 220], [58, 11012, 11, 38, 11, 7131, 11012, 11, 38, 11, 7131, 11012, 11, 38, 11, 7131, 36, 17, 34, 17, 38, 11, 17, 7131, 2943, 38, 11, 7131, 2943, 38, 11, 7131, 2943, 38, 11, 60, 930, 220], [58, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 17, 35, 17, 33, 11, 17, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 60, 930, 220], [58, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 17, 35, 17, 33, 11, 17, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 60, 930, 220], [58, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 17, 35, 17, 33, 11, 17, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 60, 930, 220], [58, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 17, 35, 17, 33, 11, 17, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 60, 930, 220], [58, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 17, 35, 17, 32, 11, 17, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 60, 930, 220], [58, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 17, 35, 17, 32, 11, 17, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 60, 930, 220], [58, 38, 2943, 7131, 38, 2943, 7131, 38, 2943, 7131, 38, 17, 36, 17, 34, 17, 7131, 38, 2943, 7131, 38, 2943, 7131, 38, 2943, 60, 930, 220], [58, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 17, 35, 17, 32, 11, 17, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 60, 930, 220], [58, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 17, 35, 17, 33, 11, 17, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 60, 930, 220], [58, 38, 2943, 7131, 38, 2943, 7131, 38, 2943, 7131, 38, 17, 36, 17, 34, 17, 7131, 38, 2943, 7131, 38, 2943, 7131, 38, 2943, 60, 930, 220], [58, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 17, 35, 17, 33, 11, 17, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 60, 930, 220], [58, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 17, 35, 17, 32, 11, 17, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 7131, 37, 5631, 11, 60, 930, 220], [58, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 17, 35, 17, 33, 11, 17, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 60, 930, 220], [58, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 17, 35, 17, 33, 11, 17, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 7131, 38, 11012, 11, 60, 930, 220]]\n",
      "/content/drive/MyDrive/test2/8856_151384.abc\n",
      "======total=====\n",
      "[[36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 38, 9832, 14, 17, 87, 14, 17, 37, 9832, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 38, 9832, 37, 9832, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 38, 9832, 14, 17, 87, 14, 17, 37, 9832, 930, 220], [36, 9832, 87, 19, 37, 9832, 36, 9832, 14, 17, 37, 9832, 18, 14, 17, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 38, 9832, 14, 17, 87, 14, 17, 37, 9832, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 38, 9832, 37, 9832, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 38, 9832, 14, 17, 87, 14, 17, 37, 9832, 930, 220], [36, 9832, 87, 19, 37, 9832, 36, 9832, 27, 37, 9832, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 38, 9832, 14, 17, 87, 14, 17, 32, 9832, 14, 17, 87, 14, 17, 930, 220], [33, 9832, 87, 21, 87, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 32, 9832, 14, 17, 87, 14, 17, 38, 9832, 14, 17, 87, 14, 17, 930, 220], [37, 9832, 87, 21, 37, 9832, 14, 17, 87, 14, 17, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 38, 9832, 14, 17, 87, 14, 17, 32, 9832, 14, 17, 87, 14, 17, 930, 220], [33, 9832, 87, 21, 87, 930, 220], [36, 9832, 87, 19, 87, 14, 17, 36, 9832, 14, 17, 32, 9832, 14, 17, 87, 14, 17, 34, 11, 14, 17, 87, 14, 17, 930, 220], [33, 9832, 87, 18, 35, 9832, 19, 930, 220]]\n",
      "/content/drive/MyDrive/test2/9260_177425.abc\n",
      "======total=====\n",
      "[[58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 37, 11, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 930, 220], [58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 34, 11, 14, 17, 60, 38, 11, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 37, 11, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 930, 220], [58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 34, 11, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 38, 11, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 38, 11, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 930, 220], [58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 87, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 58, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 36, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 34, 11, 14, 17, 34, 11, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 87, 58, 38, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 34, 11, 14, 17, 60, 38, 11, 14, 17, 58, 38, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 60, 37, 11, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 87, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 33, 9832, 14, 17, 58, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 7131, 37, 11, 14, 17, 34, 11, 14, 17, 33, 9832, 14, 17, 60, 930, 220], [58, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 60, 34, 11, 14, 17, 87, 14, 17, 58, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 7131, 38, 11, 14, 17, 36, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220], [58, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 60, 87, 14, 17, 58, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 7131, 32, 11, 14, 17, 37, 11, 14, 17, 34, 11, 14, 17, 60, 930, 220]]\n",
      "/content/drive/MyDrive/test2/8943_166528.abc\n",
      "======total=====\n",
      "[[87, 18, 14, 17, 58, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 18, 14, 17, 58, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 18, 14, 17, 58, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 87, 18, 14, 17, 58, 32, 14, 17, 37, 14, 17, 34, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 38, 14, 17, 61, 35, 14, 17, 34, 14, 17, 60, 87, 18, 14, 17, 58, 38, 14, 17, 61, 35, 14, 17, 34, 14, 17, 60, 87, 18, 14, 17, 58, 38, 14, 17, 61, 35, 14, 17, 34, 14, 17, 60, 87, 18, 14, 17, 58, 38, 14, 17, 61, 35, 14, 17, 34, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 28, 32, 14, 17, 61, 37, 14, 17, 28, 35, 14, 17, 60, 87, 18, 14, 17, 58, 32, 14, 17, 61, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 32, 14, 17, 61, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 32, 14, 17, 61, 37, 14, 17, 35, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 61, 32, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 38, 14, 17, 35, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 61, 32, 14, 17, 38, 14, 17, 61, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 38, 14, 17, 61, 35, 14, 17, 60, 87, 18, 14, 17, 58, 28, 32, 14, 17, 36, 14, 17, 61, 34, 14, 17, 60, 87, 18, 14, 17, 58, 32, 14, 17, 36, 14, 17, 61, 34, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 61, 32, 14, 17, 28, 37, 14, 17, 28, 35, 14, 17, 60, 87, 18, 14, 17, 58, 28, 32, 14, 17, 61, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 38, 14, 17, 35, 14, 17, 33, 11, 14, 17, 60, 87, 18, 14, 17, 58, 38, 14, 17, 35, 14, 17, 33, 11, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 61, 32, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 28, 32, 14, 17, 28, 37, 14, 17, 61, 35, 14, 17, 60, 87, 18, 14, 17, 58, 32, 14, 17, 37, 14, 17, 61, 35, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 28, 35, 14, 17, 60, 87, 18, 14, 17, 58, 61, 32, 14, 17, 37, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 28, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 87, 18, 14, 17, 58, 66, 14, 17, 38, 14, 17, 36, 14, 17, 60, 930, 220], [87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 87, 18, 14, 17, 58, 33, 14, 17, 38, 14, 17, 35, 14, 17, 60, 930, 220]]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "valid_data = []\n",
    "test_data = []\n",
    "\n",
    "train_data = load_dataset(TRAIN_DIR)\n",
    "# valid_data = load_dataset(VALID_DIR)\n",
    "# test_data = load_dataset(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0LaVyijXzcc"
   },
   "outputs": [],
   "source": [
    "######\n",
    "#BERT\n",
    "#####\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ABCD_BERT(Dataset):\n",
    "    def __init__(self, data,\n",
    "                 context_bars_num=8,\n",
    "                 target_bars_num=8,\n",
    "                 bos_id=2,\n",
    "                 eos_id=3,\n",
    "                 is_test=False):\n",
    "\n",
    "        self.notes = []\n",
    "        self.keys = []\n",
    "\n",
    "        for (keys, notes) in data:\n",
    "            if notes is None:\n",
    "                continue\n",
    "\n",
    "            self.keys.append(keys)\n",
    "            self.notes.append(notes)\n",
    "\n",
    "        self.context_bars_num = context_bars_num\n",
    "        self.target_bars_num = target_bars_num\n",
    "        self.bos_id = bos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        notes = self.notes[idx]\n",
    "        keys = self.keys[idx]\n",
    "\n",
    "        if not self.is_test:\n",
    "            split_indx = 12\n",
    "\n",
    "            # split notes to context (input for network) and target (that model must to generate)\n",
    "            context_notes = notes[split_indx - self.context_bars_num : split_indx]\n",
    "            target_notes = notes[split_indx: split_indx + self.target_bars_num]\n",
    "\n",
    "        else:\n",
    "            context_notes = notes\n",
    "            target_notes = []\n",
    "\n",
    "        context_tokens = [self.bos_id] + keys\n",
    "        target_tokens = [self.bos_id]\n",
    "\n",
    "        for bar in context_notes:\n",
    "            context_tokens += bar\n",
    "\n",
    "        for bar in target_notes:\n",
    "            target_tokens += bar\n",
    "\n",
    "        context_tokens += [self.eos_id]\n",
    "        target_tokens += [self.eos_id]\n",
    "\n",
    "        context_tokens = torch.tensor(context_tokens, dtype=torch.long)\n",
    "        target_tokens = torch.tensor(target_tokens, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": context_tokens,\n",
    "            \"decoder_input_ids\": target_tokens,\n",
    "            \"labels\": target_tokens,\n",
    "        }\n",
    "    def save_to_csv(self, file_path):\n",
    "        data = []\n",
    "        for idx in range(len(self)):\n",
    "            sample = self[idx]\n",
    "            data.append({\n",
    "                \"input_ids\": \" \".join(str(token.item()) for token in sample[\"input_ids\"]),\n",
    "                \"decoder_input_ids\": \" \".join(str(token.item()) for token in sample[\"decoder_input_ids\"]),\n",
    "                \"labels\": \" \".join(str(token.item()) for token in sample[\"labels\"])\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "9H1UxuxrYjMP",
    "outputId": "505b1165-41fe-4f6a-f670-e7b55c21da96"
   },
   "outputs": [],
   "source": [
    "train_dataset_2 = ABCD(train_data)\n",
    "# valid_dataset = ABCD(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDbjMSm8ZJsi"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_function(samples):\n",
    "\n",
    "    input_ids = [sample[\"input_ids\"] for sample in samples]\n",
    "    print(samples[0])\n",
    "    decoder_input_ids = [sample[\"decoder_input_ids\"] for sample in samples]\n",
    "    labels = [sample[\"labels\"] for sample in samples]\n",
    "\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True)\n",
    "    decoder_input_ids_padded = pad_sequence(decoder_input_ids, batch_first=True)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    attention_mask = input_ids_padded != 0\n",
    "    decoder_attention_mask = decoder_input_ids_padded != 0\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"decoder_input_ids\": decoder_input_ids_padded,\n",
    "        \"labels\": labels_padded,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"decoder_attention_mask\": decoder_attention_mask,\n",
    "    }\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mAuIinF0ZtfV",
    "outputId": "7a6667d8-3379-4d19-a18d-dda2f17a3605"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Music_project/output_BERT_checkpoints'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWe9_HNPZRB_"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments,TrainerCallback\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "     output_dir='/content/first_run',\n",
    "     overwrite_output_dir=True,\n",
    "     evaluation_strategy=\"epoch\",\n",
    "     gradient_accumulation_steps=8, # recheck this one\n",
    "\n",
    "     num_train_epochs=50,\n",
    "     per_device_train_batch_size=8,\n",
    "     per_device_eval_batch_size=8,\n",
    "     save_strategy = 'steps',\n",
    "     save_steps=500,\n",
    "     eval_steps=500,\n",
    "     # logging_steps = 1,\n",
    "     logging_strategy = 'epoch',\n",
    "     fp16=True,\n",
    "     report_to=\"wandb\",  # enable logging to W&B\n",
    "     run_name=\"bert-base-music_project\",\n",
    "\n",
    "     logging_dir='/content/first_run',\n",
    " )\n",
    "\n",
    "\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            print(logs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=abc_dataset,\n",
    "    # eval_dataset=valid_dataset,\n",
    "    data_collator= collate_function,\n",
    "    callbacks=[PrinterCallback],  # We can either pass the callback class this way or an instance of it (MyCallback())\n",
    "\n",
    ")\n",
    "\n",
    "# Start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0IaKjfcVHTFc",
    "outputId": "911b492c-1352-4344-cf73-a9d0a1d8fff5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32387' max='155400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 32387/155400 9:46:16 < 37:06:55, 0.92 it/s, Epoch 20.83/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1554</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3109</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4664</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6219</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7774</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9329</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10884</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15548</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18658</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21768</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23323</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24878</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26432</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27987</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29542</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31097</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0297, 'learning_rate': 1.982551546391753e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 1.9625128865979383e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 1.942474226804124e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'learning_rate': 1.9224355670103095e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 1.902396907216495e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 1.8823582474226806e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.8623195876288663e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.8422809278350517e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.8222551546391752e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.802216494845361e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.7821778350515467e-05, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.762139175257732e-05, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.7421005154639178e-05, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.7220618556701032e-05, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.702036082474227e-05, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.6819974226804124e-05, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.6619845360824746e-05, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.641958762886598e-05, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.6219201030927834e-05, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.6018943298969075e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Fifth Run\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5FPGs6YwgIvr",
    "outputId": "54318977-ddb5-4313-9673-8eed0a6120ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7850' max='7850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7850/7850 2:02:55, Epoch 49/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>5.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>3.619400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>1.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>0.364400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1259</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1416</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1573</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1731</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1888</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2045</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2203</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2518</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2832</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3147</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3304</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3462</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3619</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3777</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3934</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4091</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4249</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4406</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4563</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4721</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4878</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5036</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5193</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5508</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5665</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5822</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6137</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6295</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6452</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6609</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6767</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6924</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7081</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7239</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7396</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7554</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7711</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8465, 'learning_rate': 1.5700000000000002e-05, 'epoch': 1.0}\n",
      "{'loss': 3.6194, 'learning_rate': 1.998904336335509e-05, 'epoch': 2.0}\n",
      "{'loss': 1.4841, 'learning_rate': 1.9937679191605964e-05, 'epoch': 3.0}\n",
      "{'loss': 0.3644, 'learning_rate': 1.984521179060989e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0882, 'learning_rate': 1.9711832381924365e-05, 'epoch': 4.99}\n",
      "{'loss': 0.0443, 'learning_rate': 1.9536860733321152e-05, 'epoch': 6.0}\n",
      "{'loss': 0.029, 'learning_rate': 1.9323238012155125e-05, 'epoch': 7.0}\n",
      "{'loss': 0.0208, 'learning_rate': 1.9069142925435335e-05, 'epoch': 8.0}\n",
      "{'loss': 0.016, 'learning_rate': 1.8778846657551135e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0129, 'learning_rate': 1.845206968721005e-05, 'epoch': 10.0}\n",
      "{'loss': 0.0104, 'learning_rate': 1.8087755429170473e-05, 'epoch': 11.0}\n",
      "{'loss': 0.0088, 'learning_rate': 1.769202778528286e-05, 'epoch': 12.0}\n",
      "{'loss': 0.0075, 'learning_rate': 1.7264335740162244e-05, 'epoch': 12.99}\n",
      "{'loss': 0.0064, 'learning_rate': 1.6803447414783938e-05, 'epoch': 14.0}\n",
      "{'loss': 0.0057, 'learning_rate': 1.631711006253251e-05, 'epoch': 15.0}\n",
      "{'loss': 0.005, 'learning_rate': 1.580117729483068e-05, 'epoch': 16.0}\n",
      "{'loss': 0.0045, 'learning_rate': 1.526432162877356e-05, 'epoch': 17.0}\n",
      "{'loss': 0.004, 'learning_rate': 1.4705589951155008e-05, 'epoch': 18.0}\n",
      "{'loss': 0.0036, 'learning_rate': 1.4123563174739036e-05, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'learning_rate': 1.3528024816844712e-05, 'epoch': 20.0}\n",
      "{'loss': 0.003, 'learning_rate': 1.2917825669370118e-05, 'epoch': 20.99}\n",
      "{'loss': 0.0028, 'learning_rate': 1.2291504239353628e-05, 'epoch': 22.0}\n",
      "{'loss': 0.0026, 'learning_rate': 1.1659588610392369e-05, 'epoch': 23.0}\n",
      "{'loss': 0.0024, 'learning_rate': 1.10166912305461e-05, 'epoch': 24.0}\n",
      "{'loss': 0.0022, 'learning_rate': 1.037361881508116e-05, 'epoch': 25.0}\n",
      "{'loss': 0.0021, 'learning_rate': 9.728993817898255e-06, 'epoch': 26.0}\n",
      "{'loss': 0.0019, 'learning_rate': 9.081405621844106e-06, 'epoch': 27.0}\n",
      "{'loss': 0.0018, 'learning_rate': 8.441739791962186e-06, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0017, 'learning_rate': 7.808549348793049e-06, 'epoch': 28.99}\n",
      "{'loss': 0.0016, 'learning_rate': 7.180525243049418e-06, 'epoch': 30.0}\n",
      "{'loss': 0.0015, 'learning_rate': 6.568224179275326e-06, 'epoch': 31.0}\n",
      "{'loss': 0.0015, 'learning_rate': 5.96642583432484e-06, 'epoch': 32.0}\n",
      "{'loss': 0.0014, 'learning_rate': 5.385246073599659e-06, 'epoch': 33.0}\n",
      "{'loss': 0.0013, 'learning_rate': 4.823243030667576e-06, 'epoch': 34.0}\n",
      "{'loss': 0.0013, 'learning_rate': 4.27938331632013e-06, 'epoch': 35.0}\n",
      "{'loss': 0.0013, 'learning_rate': 3.7628088826977815e-06, 'epoch': 36.0}\n",
      "{'loss': 0.0012, 'learning_rate': 3.2721532425334933e-06, 'epoch': 36.99}\n",
      "{'loss': 0.0012, 'learning_rate': 2.8066019966134907e-06, 'epoch': 38.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'learning_rate': 2.373980779190238e-06, 'epoch': 39.0}\n",
      "{'loss': 0.0011, 'learning_rate': 1.970601171790616e-06, 'epoch': 40.0}\n",
      "{'loss': 0.0011, 'learning_rate': 1.6032437411085711e-06, 'epoch': 41.0}\n",
      "{'loss': 0.0011, 'learning_rate': 1.2707792273019049e-06, 'epoch': 42.0}\n",
      "{'loss': 0.0011, 'learning_rate': 9.728216135571323e-07, 'epoch': 43.0}\n",
      "{'loss': 0.001, 'learning_rate': 7.14379386755859e-07, 'epoch': 44.0}\n",
      "{'loss': 0.001, 'learning_rate': 4.945237734282282e-07, 'epoch': 44.99}\n",
      "{'loss': 0.001, 'learning_rate': 3.1314792140057395e-07, 'epoch': 46.0}\n",
      "{'loss': 0.001, 'learning_rate': 1.7330064880545784e-07, 'epoch': 47.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'learning_rate': 7.378965336347188e-08, 'epoch': 48.0}\n",
      "{'loss': 0.001, 'learning_rate': 1.6287654589922653e-08, 'epoch': 49.0}\n",
      "{'loss': 0.001, 'learning_rate': 0.0, 'epoch': 49.88}\n",
      "{'train_runtime': 7376.2989, 'train_samples_per_second': 68.259, 'train_steps_per_second': 1.064, 'total_flos': 9.048081898115194e+16, 'train_loss': 0.23280484066267682, 'epoch': 49.88}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7850, training_loss=0.23280484066267682, metrics={'train_runtime': 7376.2989, 'train_samples_per_second': 68.259, 'train_steps_per_second': 1.064, 'total_flos': 9.048081898115194e+16, 'train_loss': 0.23280484066267682, 'epoch': 49.88})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fourth Run\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JR78KyNQmKLz",
    "outputId": "e62b11fd-d0c6-4af4-dd7e-a14550c767c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8600' max='8600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8600/8600 2:34:41, Epoch 98/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.355100</td>\n",
       "      <td>5.505111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.473400</td>\n",
       "      <td>2.016948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.253200</td>\n",
       "      <td>0.429838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.139001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.109631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.098248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.092101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.088578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.086435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.084204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.082704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.081601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.080252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.079623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.078877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.078413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.077895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.077371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.076650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.076313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.076046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.075840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.075503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.075202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.074896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.074628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.074522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.074372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.074118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.074044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.073878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.073652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.073633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.073084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5008, 'learning_rate': 8.6e-06, 'epoch': 0.99}\n",
      "{'loss': 7.3551, 'learning_rate': 1.73e-05, 'epoch': 1.99}\n",
      "{'eval_loss': 5.505110740661621, 'eval_runtime': 7.077, 'eval_samples_per_second': 195.422, 'eval_steps_per_second': 24.445, 'epoch': 2.3}\n",
      "{'loss': 5.19, 'learning_rate': 1.999748234942507e-05, 'epoch': 2.99}\n",
      "{'loss': 3.4734, 'learning_rate': 1.9984890974505383e-05, 'epoch': 3.99}\n",
      "{'eval_loss': 2.0169477462768555, 'eval_runtime': 7.4168, 'eval_samples_per_second': 186.469, 'eval_steps_per_second': 23.326, 'epoch': 4.6}\n",
      "{'loss': 2.2175, 'learning_rate': 1.9961729363458e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2532, 'learning_rate': 1.9928022035699166e-05, 'epoch': 6.0}\n",
      "{'eval_loss': 0.42983782291412354, 'eval_runtime': 7.1236, 'eval_samples_per_second': 194.142, 'eval_steps_per_second': 24.285, 'epoch': 6.91}\n",
      "{'loss': 0.557, 'learning_rate': 1.9883804674584312e-05, 'epoch': 7.0}\n",
      "{'loss': 0.1924, 'learning_rate': 1.982912408963285e-05, 'epoch': 8.0}\n",
      "{'loss': 0.0667, 'learning_rate': 1.9764845145447687e-05, 'epoch': 8.99}\n",
      "{'eval_loss': 0.13900142908096313, 'eval_runtime': 7.4351, 'eval_samples_per_second': 186.008, 'eval_steps_per_second': 23.268, 'epoch': 9.21}\n",
      "{'loss': 0.0371, 'learning_rate': 1.9689541163440347e-05, 'epoch': 9.99}\n",
      "{'loss': 0.0254, 'learning_rate': 1.9603979609434666e-05, 'epoch': 10.99}\n",
      "{'eval_loss': 0.10963056981563568, 'eval_runtime': 7.4455, 'eval_samples_per_second': 185.75, 'eval_steps_per_second': 23.236, 'epoch': 11.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0192, 'learning_rate': 1.9508251060867252e-05, 'epoch': 11.99}\n",
      "{'loss': 0.0153, 'learning_rate': 1.9402456858189912e-05, 'epoch': 13.0}\n",
      "{'eval_loss': 0.0982479527592659, 'eval_runtime': 7.1707, 'eval_samples_per_second': 192.869, 'eval_steps_per_second': 24.126, 'epoch': 13.81}\n",
      "{'loss': 0.0125, 'learning_rate': 1.9286708997588278e-05, 'epoch': 14.0}\n",
      "{'loss': 0.0106, 'learning_rate': 1.9161130012420113e-05, 'epoch': 15.0}\n",
      "{'loss': 0.009, 'learning_rate': 1.902585284349861e-05, 'epoch': 16.0}\n",
      "{'eval_loss': 0.09210092574357986, 'eval_runtime': 7.134, 'eval_samples_per_second': 193.86, 'eval_steps_per_second': 24.25, 'epoch': 16.12}\n",
      "{'loss': 0.0079, 'learning_rate': 1.8882739150311568e-05, 'epoch': 16.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0069, 'learning_rate': 1.8728612507913358e-05, 'epoch': 17.99}\n",
      "{'eval_loss': 0.08857765048742294, 'eval_runtime': 6.7653, 'eval_samples_per_second': 204.426, 'eval_steps_per_second': 25.572, 'epoch': 18.42}\n",
      "{'loss': 0.0062, 'learning_rate': 1.8565245554778516e-05, 'epoch': 18.99}\n",
      "{'loss': 0.0055, 'learning_rate': 1.839281123493563e-05, 'epoch': 19.99}\n",
      "{'eval_loss': 0.08643537014722824, 'eval_runtime': 7.0259, 'eval_samples_per_second': 196.844, 'eval_steps_per_second': 24.623, 'epoch': 20.72}\n",
      "{'loss': 0.005, 'learning_rate': 1.821149209133704e-05, 'epoch': 21.0}\n",
      "{'loss': 0.0045, 'learning_rate': 1.8021480072614653e-05, 'epoch': 22.0}\n",
      "{'loss': 0.0041, 'learning_rate': 1.7822976329878692e-05, 'epoch': 23.0}\n",
      "{'eval_loss': 0.08420398086309433, 'eval_runtime': 7.1806, 'eval_samples_per_second': 192.602, 'eval_steps_per_second': 24.093, 'epoch': 23.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0038, 'learning_rate': 1.761619100377449e-05, 'epoch': 24.0}\n",
      "{'loss': 0.0035, 'learning_rate': 1.74038574754189e-05, 'epoch': 24.99}\n",
      "{'eval_loss': 0.08270398527383804, 'eval_runtime': 7.0157, 'eval_samples_per_second': 197.129, 'eval_steps_per_second': 24.659, 'epoch': 25.32}\n",
      "{'loss': 0.0032, 'learning_rate': 1.718126297763189e-05, 'epoch': 25.99}\n",
      "{'loss': 0.003, 'learning_rate': 1.695106622904791e-05, 'epoch': 26.99}\n",
      "{'eval_loss': 0.08160126954317093, 'eval_runtime': 7.1942, 'eval_samples_per_second': 192.24, 'eval_steps_per_second': 24.047, 'epoch': 27.63}\n",
      "{'loss': 0.0028, 'learning_rate': 1.671351092126004e-05, 'epoch': 27.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0026, 'learning_rate': 1.6468848535802043e-05, 'epoch': 29.0}\n",
      "{'eval_loss': 0.08025236427783966, 'eval_runtime': 7.0675, 'eval_samples_per_second': 195.684, 'eval_steps_per_second': 24.478, 'epoch': 29.93}\n",
      "{'loss': 0.0024, 'learning_rate': 1.62173380779242e-05, 'epoch': 30.0}\n",
      "{'loss': 0.0023, 'learning_rate': 1.5959245802404365e-05, 'epoch': 31.0}\n",
      "{'loss': 0.0021, 'learning_rate': 1.569484493168452e-05, 'epoch': 32.0}\n",
      "{'eval_loss': 0.07962320744991302, 'eval_runtime': 7.6711, 'eval_samples_per_second': 180.287, 'eval_steps_per_second': 22.552, 'epoch': 32.23}\n",
      "{'loss': 0.002, 'learning_rate': 1.5427556929731312e-05, 'epoch': 32.99}\n",
      "{'loss': 0.0019, 'learning_rate': 1.515144930847762e-05, 'epoch': 33.99}\n",
      "{'eval_loss': 0.07887732982635498, 'eval_runtime': 7.525, 'eval_samples_per_second': 183.787, 'eval_steps_per_second': 22.99, 'epoch': 34.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 1.4869888244043674e-05, 'epoch': 34.99}\n",
      "{'loss': 0.0017, 'learning_rate': 1.4583171803473279e-05, 'epoch': 35.99}\n",
      "{'eval_loss': 0.07841328531503677, 'eval_runtime': 7.2776, 'eval_samples_per_second': 190.036, 'eval_steps_per_second': 23.772, 'epoch': 36.83}\n",
      "{'loss': 0.0016, 'learning_rate': 1.4291603511410449e-05, 'epoch': 37.0}\n",
      "{'loss': 0.0015, 'learning_rate': 1.3995492028781202e-05, 'epoch': 38.0}\n",
      "{'loss': 0.0014, 'learning_rate': 1.3695150826037998e-05, 'epoch': 39.0}\n",
      "{'eval_loss': 0.07789459079504013, 'eval_runtime': 7.1169, 'eval_samples_per_second': 194.328, 'eval_steps_per_second': 24.309, 'epoch': 39.14}\n",
      "{'loss': 0.0014, 'learning_rate': 1.3390897851312667e-05, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 1.3086612784659842e-05, 'epoch': 40.99}\n",
      "{'eval_loss': 0.07737118750810623, 'eval_runtime': 8.9371, 'eval_samples_per_second': 154.748, 'eval_steps_per_second': 19.358, 'epoch': 41.44}\n",
      "{'loss': 0.0013, 'learning_rate': 1.2775541983889333e-05, 'epoch': 41.99}\n",
      "{'loss': 0.0012, 'learning_rate': 1.2461532930289932e-05, 'epoch': 42.99}\n",
      "{'eval_loss': 0.07664971798658371, 'eval_runtime': 7.0807, 'eval_samples_per_second': 195.319, 'eval_steps_per_second': 24.433, 'epoch': 43.74}\n",
      "{'loss': 0.0011, 'learning_rate': 1.214491804109596e-05, 'epoch': 43.99}\n",
      "{'loss': 0.0011, 'learning_rate': 1.1826032492139474e-05, 'epoch': 45.0}\n",
      "{'loss': 0.0011, 'learning_rate': 1.150521386302537e-05, 'epoch': 46.0}\n",
      "{'eval_loss': 0.07631289213895798, 'eval_runtime': 6.9861, 'eval_samples_per_second': 197.964, 'eval_steps_per_second': 24.763, 'epoch': 46.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'learning_rate': 1.118280177976185e-05, 'epoch': 47.0}\n",
      "{'loss': 0.001, 'learning_rate': 1.0859137555224448e-05, 'epoch': 48.0}\n",
      "{'eval_loss': 0.07604631781578064, 'eval_runtime': 7.2168, 'eval_samples_per_second': 191.637, 'eval_steps_per_second': 23.972, 'epoch': 48.35}\n",
      "{'loss': 0.0009, 'learning_rate': 1.0538298434121284e-05, 'epoch': 48.99}\n",
      "{'loss': 0.0009, 'learning_rate': 1.0213163355112147e-05, 'epoch': 49.99}\n",
      "{'eval_loss': 0.07584027945995331, 'eval_runtime': 7.3157, 'eval_samples_per_second': 189.045, 'eval_steps_per_second': 23.648, 'epoch': 50.65}\n",
      "{'loss': 0.0009, 'learning_rate': 9.887802616453543e-06, 'epoch': 50.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'learning_rate': 9.562560652535695e-06, 'epoch': 51.99}\n",
      "{'eval_loss': 0.0755033865571022, 'eval_runtime': 7.198, 'eval_samples_per_second': 192.137, 'eval_steps_per_second': 24.035, 'epoch': 52.95}\n",
      "{'loss': 0.0008, 'learning_rate': 9.237781772011152e-06, 'epoch': 53.0}\n",
      "{'loss': 0.0008, 'learning_rate': 8.913809793301682e-06, 'epoch': 54.0}\n",
      "{'loss': 0.0008, 'learning_rate': 8.590987680624174e-06, 'epoch': 55.0}\n",
      "{'eval_loss': 0.07520244270563126, 'eval_runtime': 7.4946, 'eval_samples_per_second': 184.533, 'eval_steps_per_second': 23.083, 'epoch': 55.25}\n",
      "{'loss': 0.0007, 'learning_rate': 8.269657180920773e-06, 'epoch': 56.0}\n",
      "{'loss': 0.0007, 'learning_rate': 7.953819178985326e-06, 'epoch': 56.99}\n",
      "{'eval_loss': 0.07489626109600067, 'eval_runtime': 11.2975, 'eval_samples_per_second': 122.417, 'eval_steps_per_second': 15.313, 'epoch': 57.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 7.636463613895024e-06, 'epoch': 57.99}\n",
      "{'loss': 0.0007, 'learning_rate': 7.321610142994971e-06, 'epoch': 58.99}\n",
      "{'eval_loss': 0.07462754845619202, 'eval_runtime': 7.1094, 'eval_samples_per_second': 194.532, 'eval_steps_per_second': 24.334, 'epoch': 59.86}\n",
      "{'loss': 0.0007, 'learning_rate': 7.009592077439135e-06, 'epoch': 59.99}\n",
      "{'loss': 0.0006, 'learning_rate': 6.700739726755931e-06, 'epoch': 61.0}\n",
      "{'loss': 0.0006, 'learning_rate': 6.3953800491749095e-06, 'epoch': 62.0}\n",
      "{'eval_loss': 0.07452213019132614, 'eval_runtime': 7.1468, 'eval_samples_per_second': 193.514, 'eval_steps_per_second': 24.207, 'epoch': 62.16}\n",
      "{'loss': 0.0006, 'learning_rate': 6.093836305501242e-06, 'epoch': 63.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 5.796427716904347e-06, 'epoch': 64.0}\n",
      "{'eval_loss': 0.07437185943126678, 'eval_runtime': 8.5204, 'eval_samples_per_second': 162.317, 'eval_steps_per_second': 20.304, 'epoch': 64.46}\n",
      "{'loss': 0.0006, 'learning_rate': 5.506810013841036e-06, 'epoch': 64.99}\n",
      "{'loss': 0.0006, 'learning_rate': 5.218555097949634e-06, 'epoch': 65.99}\n",
      "{'eval_loss': 0.07411817461252213, 'eval_runtime': 7.2644, 'eval_samples_per_second': 190.381, 'eval_steps_per_second': 23.815, 'epoch': 66.76}\n",
      "{'loss': 0.0005, 'learning_rate': 4.935361930030774e-06, 'epoch': 66.99}\n",
      "{'loss': 0.0005, 'learning_rate': 4.657530304910679e-06, 'epoch': 67.99}\n",
      "{'loss': 0.0005, 'learning_rate': 4.385354341562596e-06, 'epoch': 69.0}\n",
      "{'eval_loss': 0.07404367625713348, 'eval_runtime': 10.3702, 'eval_samples_per_second': 133.363, 'eval_steps_per_second': 16.682, 'epoch': 69.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 4.119122171745608e-06, 'epoch': 70.0}\n",
      "{'loss': 0.0005, 'learning_rate': 3.859115634981748e-06, 'epoch': 71.0}\n",
      "{'eval_loss': 0.0738782212138176, 'eval_runtime': 7.2946, 'eval_samples_per_second': 189.593, 'eval_steps_per_second': 23.716, 'epoch': 71.37}\n",
      "{'loss': 0.0005, 'learning_rate': 3.6056099801941535e-06, 'epoch': 72.0}\n",
      "{'loss': 0.0005, 'learning_rate': 3.361670177840707e-06, 'epoch': 72.99}\n",
      "{'eval_loss': 0.07365242391824722, 'eval_runtime': 7.3639, 'eval_samples_per_second': 187.809, 'eval_steps_per_second': 23.493, 'epoch': 73.67}\n",
      "{'loss': 0.0005, 'learning_rate': 3.121881955840421e-06, 'epoch': 73.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 2.8893750684111977e-06, 'epoch': 74.99}\n",
      "{'eval_loss': 0.0736328661441803, 'eval_runtime': 7.2368, 'eval_samples_per_second': 191.107, 'eval_steps_per_second': 23.906, 'epoch': 75.97}\n",
      "{'loss': 0.0005, 'learning_rate': 2.664395652712435e-06, 'epoch': 75.99}\n",
      "{'loss': 0.0005, 'learning_rate': 2.447181877148165e-06, 'epoch': 77.0}\n",
      "{'loss': 0.0004, 'learning_rate': 2.237963689236472e-06, 'epoch': 78.0}\n",
      "{'eval_loss': 0.07348404079675674, 'eval_runtime': 8.975, 'eval_samples_per_second': 154.095, 'eval_steps_per_second': 19.276, 'epoch': 78.27}\n",
      "{'loss': 0.0004, 'learning_rate': 2.036962572181731e-06, 'epoch': 79.0}\n",
      "{'loss': 0.0004, 'learning_rate': 1.8443913104073984e-06, 'epoch': 80.0}\n",
      "{'eval_loss': 0.07343505322933197, 'eval_runtime': 7.3541, 'eval_samples_per_second': 188.059, 'eval_steps_per_second': 23.524, 'epoch': 80.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 1.662518198179528e-06, 'epoch': 80.99}\n",
      "{'loss': 0.0004, 'learning_rate': 1.487306540771315e-06, 'epoch': 81.99}\n",
      "{'eval_loss': 0.07323481142520905, 'eval_runtime': 6.8565, 'eval_samples_per_second': 201.705, 'eval_steps_per_second': 25.231, 'epoch': 82.88}\n",
      "{'loss': 0.0004, 'learning_rate': 1.3211066172094178e-06, 'epoch': 82.99}\n",
      "{'loss': 0.0004, 'learning_rate': 1.1640943705703256e-06, 'epoch': 83.99}\n",
      "{'loss': 0.0004, 'learning_rate': 1.0164360176435962e-06, 'epoch': 85.0}\n",
      "{'eval_loss': 0.07321203500032425, 'eval_runtime': 7.197, 'eval_samples_per_second': 192.165, 'eval_steps_per_second': 24.038, 'epoch': 85.18}\n",
      "{'loss': 0.0004, 'learning_rate': 8.782878729709399e-07, 'epoch': 86.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 7.49796183368019e-07, 'epoch': 87.0}\n",
      "{'eval_loss': 0.07315260916948318, 'eval_runtime': 7.5284, 'eval_samples_per_second': 183.703, 'eval_steps_per_second': 22.98, 'epoch': 87.48}\n",
      "{'loss': 0.0004, 'learning_rate': 6.31096973104206e-07, 'epoch': 88.0}\n",
      "{'loss': 0.0004, 'learning_rate': 5.235094677507402e-07, 'epoch': 88.99}\n",
      "{'eval_loss': 0.0731230154633522, 'eval_runtime': 7.3891, 'eval_samples_per_second': 187.169, 'eval_steps_per_second': 23.413, 'epoch': 89.78}\n",
      "{'loss': 0.0004, 'learning_rate': 4.246457502031631e-07, 'epoch': 89.99}\n",
      "{'loss': 0.0004, 'learning_rate': 3.359187237506689e-07, 'epoch': 90.99}\n",
      "{'loss': 0.0004, 'learning_rate': 2.5742231687209016e-07, 'epoch': 91.99}\n",
      "{'eval_loss': 0.07306259870529175, 'eval_runtime': 6.9056, 'eval_samples_per_second': 200.273, 'eval_steps_per_second': 25.052, 'epoch': 92.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 1.8923962767615545e-07, 'epoch': 93.0}\n",
      "{'loss': 0.0004, 'learning_rate': 1.3144283593192752e-07, 'epoch': 94.0}\n",
      "{'eval_loss': 0.0730486586689949, 'eval_runtime': 7.4922, 'eval_samples_per_second': 184.593, 'eval_steps_per_second': 23.091, 'epoch': 94.39}\n",
      "{'loss': 0.0004, 'learning_rate': 8.40931266576206e-08, 'epoch': 95.0}\n",
      "{'loss': 0.0004, 'learning_rate': 4.7240625348735636e-08, 'epoch': 96.0}\n",
      "{'eval_loss': 0.07308094203472137, 'eval_runtime': 7.2017, 'eval_samples_per_second': 192.037, 'eval_steps_per_second': 24.022, 'epoch': 96.69}\n",
      "{'loss': 0.0004, 'learning_rate': 2.1166858367646092e-08, 'epoch': 96.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 5.293115445467179e-09, 'epoch': 97.99}\n",
      "{'eval_loss': 0.0730840414762497, 'eval_runtime': 7.0677, 'eval_samples_per_second': 195.679, 'eval_steps_per_second': 24.478, 'epoch': 98.99}\n",
      "{'loss': 0.0004, 'learning_rate': 0.0, 'epoch': 98.99}\n",
      "{'train_runtime': 9283.0026, 'train_samples_per_second': 59.873, 'train_steps_per_second': 0.926, 'total_flos': 1.496030941481132e+17, 'train_loss': 0.30288746738893, 'epoch': 98.99}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8600, training_loss=0.30288746738893, metrics={'train_runtime': 9283.0026, 'train_samples_per_second': 59.873, 'train_steps_per_second': 0.926, 'total_flos': 1.496030941481132e+17, 'train_loss': 0.30288746738893, 'epoch': 98.99})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Third Run\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ax089SYPZ22-",
    "outputId": "43f3d5ea-31a9-46fb-f00b-9156f81405b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madam-mourad1960\u001b[0m (\u001b[33mmusic_project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20230618_121001-w0rwoxjy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/music_project/huggingface/runs/w0rwoxjy' target=\"_blank\">bert-base-music_project</a></strong> to <a href='https://wandb.ai/music_project/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/music_project/huggingface' target=\"_blank\">https://wandb.ai/music_project/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/music_project/huggingface/runs/w0rwoxjy' target=\"_blank\">https://wandb.ai/music_project/huggingface/runs/w0rwoxjy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1050' max='1050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1050/1050 43:04, Epoch 48/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.199700</td>\n",
       "      <td>5.350147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.225500</td>\n",
       "      <td>1.906656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.549400</td>\n",
       "      <td>0.511615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.217047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.181315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.6595, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.97}\n",
      "{'loss': 9.6048, 'learning_rate': 4.3e-06, 'epoch': 1.98}\n",
      "{'loss': 9.1835, 'learning_rate': 6.5000000000000004e-06, 'epoch': 2.99}\n",
      "{'loss': 8.7154, 'learning_rate': 8.700000000000001e-06, 'epoch': 4.0}\n",
      "{'loss': 8.6013, 'learning_rate': 1.0800000000000002e-05, 'epoch': 4.97}\n",
      "{'loss': 7.6603, 'learning_rate': 1.3000000000000001e-05, 'epoch': 5.98}\n",
      "{'loss': 7.0804, 'learning_rate': 1.5200000000000002e-05, 'epoch': 6.99}\n",
      "{'loss': 6.4889, 'learning_rate': 1.7400000000000003e-05, 'epoch': 8.0}\n",
      "{'loss': 6.1997, 'learning_rate': 1.95e-05, 'epoch': 8.97}\n",
      "{'eval_loss': 5.350146770477295, 'eval_runtime': 4.3195, 'eval_samples_per_second': 320.177, 'eval_steps_per_second': 20.141, 'epoch': 9.2}\n",
      "{'loss': 5.3507, 'learning_rate': 1.9980267284282718e-05, 'epoch': 9.98}\n",
      "{'loss': 4.8314, 'learning_rate': 1.9896292772724142e-05, 'epoch': 10.99}\n",
      "{'loss': 4.3184, 'learning_rate': 1.974692387082714e-05, 'epoch': 12.0}\n",
      "{'loss': 4.0791, 'learning_rate': 1.9544243561566404e-05, 'epoch': 12.97}\n",
      "{'loss': 3.4883, 'learning_rate': 1.9270299383067497e-05, 'epoch': 13.98}\n",
      "{'loss': 3.1263, 'learning_rate': 1.8935097358794144e-05, 'epoch': 14.99}\n",
      "{'loss': 2.7984, 'learning_rate': 1.8540852493144546e-05, 'epoch': 16.0}\n",
      "{'loss': 2.6097, 'learning_rate': 1.8111839129190087e-05, 'epoch': 16.97}\n",
      "{'loss': 2.2255, 'learning_rate': 1.7610057295013896e-05, 'epoch': 17.98}\n",
      "{'eval_loss': 1.9066561460494995, 'eval_runtime': 4.2822, 'eval_samples_per_second': 322.967, 'eval_steps_per_second': 20.317, 'epoch': 18.39}\n",
      "{'loss': 1.9516, 'learning_rate': 1.705798844238777e-05, 'epoch': 18.99}\n",
      "{'loss': 1.701, 'learning_rate': 1.6459280624867876e-05, 'epoch': 20.0}\n",
      "{'loss': 1.5584, 'learning_rate': 1.584791124608955e-05, 'epoch': 20.97}\n",
      "{'loss': 1.2835, 'learning_rate': 1.5169728102004256e-05, 'epoch': 21.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0956, 'learning_rate': 1.4457383557765385e-05, 'epoch': 22.99}\n",
      "{'loss': 0.9336, 'learning_rate': 1.3715584763641345e-05, 'epoch': 24.0}\n",
      "{'loss': 0.8207, 'learning_rate': 1.2984529248893081e-05, 'epoch': 24.97}\n",
      "{'loss': 0.6598, 'learning_rate': 1.2199463578396688e-05, 'epoch': 25.98}\n",
      "{'loss': 0.5494, 'learning_rate': 1.1399863921984151e-05, 'epoch': 26.99}\n",
      "{'eval_loss': 0.5116150975227356, 'eval_runtime': 4.2557, 'eval_samples_per_second': 324.973, 'eval_steps_per_second': 20.443, 'epoch': 27.59}\n",
      "{'loss': 0.4617, 'learning_rate': 1.0591014008951555e-05, 'epoch': 28.0}\n",
      "{'loss': 0.4071, 'learning_rate': 9.815210950408703e-06, 'epoch': 28.97}\n",
      "{'loss': 0.3298, 'learning_rate': 9.00373778573246e-06, 'epoch': 29.98}\n",
      "{'loss': 0.282, 'learning_rate': 8.198847890328405e-06, 'epoch': 30.99}\n",
      "{'loss': 0.2425, 'learning_rate': 7.4058599512249345e-06, 'epoch': 32.0}\n",
      "{'loss': 0.2208, 'learning_rate': 6.664834894950232e-06, 'epoch': 32.97}\n",
      "{'loss': 0.1854, 'learning_rate': 5.91013623160902e-06, 'epoch': 33.98}\n",
      "{'loss': 0.165, 'learning_rate': 5.1824632589828465e-06, 'epoch': 34.99}\n",
      "{'loss': 0.149, 'learning_rate': 4.486624417111179e-06, 'epoch': 36.0}\n",
      "{'eval_loss': 0.21704675257205963, 'eval_runtime': 4.2231, 'eval_samples_per_second': 327.486, 'eval_steps_per_second': 20.601, 'epoch': 36.78}\n",
      "{'loss': 0.1419, 'learning_rate': 3.856337906953363e-06, 'epoch': 36.97}\n",
      "{'loss': 0.1254, 'learning_rate': 3.235775934557204e-06, 'epoch': 37.98}\n",
      "{'loss': 0.1174, 'learning_rate': 2.659911741322482e-06, 'epoch': 38.99}\n",
      "{'loss': 0.1111, 'learning_rate': 2.132550619665168e-06, 'epoch': 40.0}\n",
      "{'loss': 0.1109, 'learning_rate': 1.6776116802269714e-06, 'epoch': 40.97}\n",
      "{'loss': 0.1023, 'learning_rate': 1.2547985864015144e-06, 'epoch': 41.98}\n",
      "{'loss': 0.0999, 'learning_rate': 8.897735075391156e-07, 'epoch': 42.99}\n",
      "{'loss': 0.0979, 'learning_rate': 5.849485178415671e-07, 'epoch': 44.0}\n",
      "{'loss': 0.1011, 'learning_rate': 3.5199175204913116e-07, 'epoch': 44.97}\n",
      "{'eval_loss': 0.18131476640701294, 'eval_runtime': 4.1751, 'eval_samples_per_second': 331.249, 'eval_steps_per_second': 20.838, 'epoch': 45.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0961, 'learning_rate': 1.7026900316098217e-07, 'epoch': 45.98}\n",
      "{'loss': 0.0955, 'learning_rate': 5.3500806496741276e-08, 'epoch': 46.99}\n",
      "{'loss': 0.0954, 'learning_rate': 2.458762615035193e-09, 'epoch': 48.0}\n",
      "{'loss': 0.0963, 'learning_rate': 0.0, 'epoch': 48.28}\n",
      "{'train_runtime': 2595.9894, 'train_samples_per_second': 107.05, 'train_steps_per_second': 0.404, 'total_flos': 8.114630532889133e+16, 'train_loss': 2.508532467399325, 'epoch': 48.28}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1050, training_loss=2.508532467399325, metrics={'train_runtime': 2595.9894, 'train_samples_per_second': 107.05, 'train_steps_per_second': 0.404, 'total_flos': 8.114630532889133e+16, 'train_loss': 2.508532467399325, 'epoch': 48.28})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Second Run\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xg-m_FaOaJoq",
    "outputId": "e3288a2b-3641-4752-9720-631b44c7352d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3450' max='3450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3450/3450 54:54, Epoch 49/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.178636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.158075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.152992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.151644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.149198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.148097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3521, 'learning_rate': 4.9e-05, 'epoch': 0.99}\n",
      "{'loss': 0.2353, 'learning_rate': 4.7985507246376815e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0558, 'learning_rate': 4.698550724637682e-05, 'epoch': 2.99}\n",
      "{'loss': 0.0282, 'learning_rate': 4.597101449275363e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0184, 'learning_rate': 4.497101449275363e-05, 'epoch': 4.99}\n",
      "{'loss': 0.013, 'learning_rate': 4.395652173913043e-05, 'epoch': 6.0}\n",
      "{'loss': 0.0101, 'learning_rate': 4.2956521739130435e-05, 'epoch': 6.99}\n",
      "{'eval_loss': 0.178636372089386, 'eval_runtime': 5.9065, 'eval_samples_per_second': 188.267, 'eval_steps_per_second': 23.533, 'epoch': 7.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0079, 'learning_rate': 4.194202898550725e-05, 'epoch': 8.0}\n",
      "{'loss': 0.0066, 'learning_rate': 4.094202898550725e-05, 'epoch': 8.99}\n",
      "{'loss': 0.0054, 'learning_rate': 3.9927536231884064e-05, 'epoch': 10.0}\n",
      "{'loss': 0.0047, 'learning_rate': 3.892753623188406e-05, 'epoch': 10.99}\n",
      "{'loss': 0.0041, 'learning_rate': 3.7913043478260876e-05, 'epoch': 12.0}\n",
      "{'loss': 0.0036, 'learning_rate': 3.691304347826087e-05, 'epoch': 12.99}\n",
      "{'loss': 0.0032, 'learning_rate': 3.589855072463768e-05, 'epoch': 14.0}\n",
      "{'eval_loss': 0.1580747365951538, 'eval_runtime': 5.7716, 'eval_samples_per_second': 192.668, 'eval_steps_per_second': 24.084, 'epoch': 14.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0029, 'learning_rate': 3.4898550724637684e-05, 'epoch': 14.99}\n",
      "{'loss': 0.0026, 'learning_rate': 3.3884057971014493e-05, 'epoch': 16.0}\n",
      "{'loss': 0.0024, 'learning_rate': 3.288405797101449e-05, 'epoch': 16.99}\n",
      "{'loss': 0.0022, 'learning_rate': 3.1869565217391306e-05, 'epoch': 18.0}\n",
      "{'loss': 0.002, 'learning_rate': 3.086956521739131e-05, 'epoch': 18.99}\n",
      "{'loss': 0.0019, 'learning_rate': 2.9855072463768118e-05, 'epoch': 20.0}\n",
      "{'loss': 0.0018, 'learning_rate': 2.8855072463768117e-05, 'epoch': 20.99}\n",
      "{'eval_loss': 0.152992382645607, 'eval_runtime': 5.91, 'eval_samples_per_second': 188.156, 'eval_steps_per_second': 23.519, 'epoch': 21.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'learning_rate': 2.7840579710144927e-05, 'epoch': 22.0}\n",
      "{'loss': 0.0016, 'learning_rate': 2.684057971014493e-05, 'epoch': 22.99}\n",
      "{'loss': 0.0015, 'learning_rate': 2.582608695652174e-05, 'epoch': 24.0}\n",
      "{'loss': 0.0014, 'learning_rate': 2.4826086956521742e-05, 'epoch': 24.99}\n",
      "{'loss': 0.0013, 'learning_rate': 2.381159420289855e-05, 'epoch': 26.0}\n",
      "{'loss': 0.0012, 'learning_rate': 2.281159420289855e-05, 'epoch': 26.99}\n",
      "{'loss': 0.0012, 'learning_rate': 2.1797101449275363e-05, 'epoch': 28.0}\n",
      "{'eval_loss': 0.15164361894130707, 'eval_runtime': 5.586, 'eval_samples_per_second': 199.068, 'eval_steps_per_second': 24.883, 'epoch': 28.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 2.0797101449275363e-05, 'epoch': 28.99}\n",
      "{'loss': 0.0011, 'learning_rate': 1.9782608695652176e-05, 'epoch': 30.0}\n",
      "{'loss': 0.0011, 'learning_rate': 1.8782608695652175e-05, 'epoch': 30.99}\n",
      "{'loss': 0.001, 'learning_rate': 1.7768115942028988e-05, 'epoch': 32.0}\n",
      "{'loss': 0.001, 'learning_rate': 1.6768115942028987e-05, 'epoch': 32.99}\n",
      "{'loss': 0.0009, 'learning_rate': 1.5753623188405797e-05, 'epoch': 34.0}\n",
      "{'loss': 0.0009, 'learning_rate': 1.47536231884058e-05, 'epoch': 34.99}\n",
      "{'eval_loss': 0.14919769763946533, 'eval_runtime': 5.6117, 'eval_samples_per_second': 198.159, 'eval_steps_per_second': 24.77, 'epoch': 35.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 1.373913043478261e-05, 'epoch': 36.0}\n",
      "{'loss': 0.0009, 'learning_rate': 1.2739130434782608e-05, 'epoch': 36.99}\n",
      "{'loss': 0.0008, 'learning_rate': 1.1724637681159421e-05, 'epoch': 38.0}\n",
      "{'loss': 0.0008, 'learning_rate': 1.072463768115942e-05, 'epoch': 38.99}\n",
      "{'loss': 0.0008, 'learning_rate': 9.710144927536233e-06, 'epoch': 40.0}\n",
      "{'loss': 0.0008, 'learning_rate': 8.710144927536231e-06, 'epoch': 40.99}\n",
      "{'loss': 0.0008, 'learning_rate': 7.695652173913044e-06, 'epoch': 42.0}\n",
      "{'loss': 0.0007, 'learning_rate': 6.695652173913043e-06, 'epoch': 42.99}\n",
      "{'eval_loss': 0.14809656143188477, 'eval_runtime': 5.6153, 'eval_samples_per_second': 198.031, 'eval_steps_per_second': 24.754, 'epoch': 43.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 5.681159420289855e-06, 'epoch': 44.0}\n",
      "{'loss': 0.0007, 'learning_rate': 4.6811594202898555e-06, 'epoch': 44.99}\n",
      "{'loss': 0.0007, 'learning_rate': 3.666666666666667e-06, 'epoch': 46.0}\n",
      "{'loss': 0.0007, 'learning_rate': 2.666666666666667e-06, 'epoch': 46.99}\n",
      "{'loss': 0.0007, 'learning_rate': 1.6521739130434782e-06, 'epoch': 48.0}\n",
      "{'loss': 0.0007, 'learning_rate': 6.521739130434782e-07, 'epoch': 48.99}\n",
      "{'loss': 0.0007, 'learning_rate': 0.0, 'epoch': 49.64}\n",
      "{'train_runtime': 3295.4549, 'train_samples_per_second': 67.457, 'train_steps_per_second': 1.047, 'total_flos': 4.15398998828304e+16, 'train_loss': 0.035935078265442365, 'epoch': 49.64}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3450, training_loss=0.035935078265442365, metrics={'train_runtime': 3295.4549, 'train_samples_per_second': 67.457, 'train_steps_per_second': 1.047, 'total_flos': 4.15398998828304e+16, 'train_loss': 0.035935078265442365, 'epoch': 49.64})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First RUN\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eee-AligaKHx",
    "outputId": "0d2d0d96-a740-4bd8-9a2c-e55fc27f2ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 15 13:12:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0    23W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "id": "zzRoj_Ov-kBs",
    "outputId": "48ec0977-6fdb-4082-e99b-c0ef8be036f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▆▆██████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>49.88</td></tr><tr><td>train/global_step</td><td>7850</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.001</td></tr><tr><td>train/total_flos</td><td>9.048081898115194e+16</td></tr><tr><td>train/train_loss</td><td>0.2328</td></tr><tr><td>train/train_runtime</td><td>7376.2989</td></tr><tr><td>train/train_samples_per_second</td><td>68.259</td></tr><tr><td>train/train_steps_per_second</td><td>1.064</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert-base-music_project(8|8)_newDS</strong> at: <a href='https://wandb.ai/music_project/huggingface/runs/m7jrz84g' target=\"_blank\">https://wandb.ai/music_project/huggingface/runs/m7jrz84g</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230621_194924-m7jrz84g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fourth RUN\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "LYqHSJodO39j",
    "outputId": "3e3d30f2-89cb-429b-eb83-8bb6b1e2834a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▂▂▂▂▁▁▂▁▂▁▂▂▂▄▁▁▂▂▂▂█▂▂▂▇▂▂▂▄▂▁▂▂▂▁▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▆▇▆▆▇▇█▇▇▇▇▇▆▇▇▄▇▇▇▇▇▆▁▇▇▇▂▇▇▇▄▇█▇▆▇█▆▇</td></tr><tr><td>eval/steps_per_second</td><td>▇▆▇▆▆▇▇█▇▇▇▇▇▆▇▇▄▇▇▇▇▇▆▁▇▇▇▂▇▇▇▄▇█▇▆▇█▆▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▄███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.07308</td></tr><tr><td>eval/runtime</td><td>7.0677</td></tr><tr><td>eval/samples_per_second</td><td>195.679</td></tr><tr><td>eval/steps_per_second</td><td>24.478</td></tr><tr><td>train/epoch</td><td>98.99</td></tr><tr><td>train/global_step</td><td>8600</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0004</td></tr><tr><td>train/total_flos</td><td>1.496030941481132e+17</td></tr><tr><td>train/train_loss</td><td>0.30289</td></tr><tr><td>train/train_runtime</td><td>9283.0026</td></tr><tr><td>train/train_samples_per_second</td><td>59.873</td></tr><tr><td>train/train_steps_per_second</td><td>0.926</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert-base-fine-tune</strong> at: <a href='https://wandb.ai/music_project/huggingface/runs/sq9ygs1p' target=\"_blank\">https://wandb.ai/music_project/huggingface/runs/sq9ygs1p</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230618_130139-sq9ygs1p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Third RUN\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "v-Sd3snIktK-",
    "outputId": "188987d8-f00f-42bb-d759-3e4e149ea76d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▆▅▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▃▄▆█</td></tr><tr><td>eval/steps_per_second</td><td>▁▃▄▆█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>▂▃▃▄▅▆▇██████▇▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▇▇▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.18131</td></tr><tr><td>eval/runtime</td><td>4.1751</td></tr><tr><td>eval/samples_per_second</td><td>331.249</td></tr><tr><td>eval/steps_per_second</td><td>20.838</td></tr><tr><td>train/epoch</td><td>48.28</td></tr><tr><td>train/global_step</td><td>1050</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0963</td></tr><tr><td>train/total_flos</td><td>8.114630532889133e+16</td></tr><tr><td>train/train_loss</td><td>2.50853</td></tr><tr><td>train/train_runtime</td><td>2595.9894</td></tr><tr><td>train/train_samples_per_second</td><td>107.05</td></tr><tr><td>train/train_steps_per_second</td><td>0.404</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert-base-music_project</strong> at: <a href='https://wandb.ai/music_project/huggingface/runs/w0rwoxjy' target=\"_blank\">https://wandb.ai/music_project/huggingface/runs/w0rwoxjy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230618_121001-w0rwoxjy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SECOND RUN\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "_y-LjmZMaP8p",
    "outputId": "0b1c50d5-2717-412b-b9db-8b490c9a7fb5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▂▁▁</td></tr><tr><td>eval/runtime</td><td>█▅█▁▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▄▁█▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▄▁█▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.1481</td></tr><tr><td>eval/runtime</td><td>5.6153</td></tr><tr><td>eval/samples_per_second</td><td>198.031</td></tr><tr><td>eval/steps_per_second</td><td>24.754</td></tr><tr><td>train/epoch</td><td>49.64</td></tr><tr><td>train/global_step</td><td>3450</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0007</td></tr><tr><td>train/total_flos</td><td>4.15398998828304e+16</td></tr><tr><td>train/train_loss</td><td>0.03594</td></tr><tr><td>train/train_runtime</td><td>3295.4549</td></tr><tr><td>train/train_samples_per_second</td><td>67.457</td></tr><tr><td>train/train_steps_per_second</td><td>1.047</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert-base-music_project</strong> at: <a href='https://wandb.ai/music_project/huggingface/runs/y6fjl160' target=\"_blank\">https://wandb.ai/music_project/huggingface/runs/y6fjl160</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230615_132012-y6fjl160/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FIRST RUN\n",
    "wandb.finish()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e7f6005b0884dab98668f33a40b6424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab006f19113f4bd29863a2c5aa9c3142",
       "IPY_MODEL_cb32a5cd024f47f79bea73278badaf47",
       "IPY_MODEL_6ea1ae76f2df4194b313b29d0200edd2"
      ],
      "layout": "IPY_MODEL_7306b2f2693441dd98f6a7fa706493ea"
     }
    },
    "1510121315844ecca315061ed797ca55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1ffb6c52e354ce3888a67004091a066",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92a8c88c88ca4d4bb9f8d3e820da7886",
      "value": 1042301
     }
    },
    "1919a8349edf4e379a91e37f60fd2e5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1da0cb6f46a84960918724301fabeb71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1e3173bf62c74f539c7dd776a287bf6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27b8a763070c4cbf8eff025dd1766704": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d12da8b2b364a06a70615fdb9e0a8e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2eda110549e9406a831faf14c2b1d3ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f33084d07f64b6b8741ad8fd0e036ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f43b6d5c769e4416b9969afbf9c77569",
      "placeholder": "​",
      "style": "IPY_MODEL_eb616ed9d31a4f7d816a2dd0b33e9339",
      "value": "Downloading (…)olve/main/vocab.json: 100%"
     }
    },
    "35ae5044ae744e808d0a6c6046694603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d8200500e584c27a764ef0dd9dbd904",
       "IPY_MODEL_7c0635408d43442b828818e88dd4b43b",
       "IPY_MODEL_76390fe0464f4eeab83b4dc1130c025e"
      ],
      "layout": "IPY_MODEL_a76e6264d2a045caabc1831d136cc980"
     }
    },
    "3d00e43236d34dc58bade00be5feb1ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42e07609569d4f8db4b3d5b52ab7d402": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "482fc7efcdc64746afe7309010ac23ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f9075bd71c748abbc8b0aab6ea9d638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ea1ae76f2df4194b313b29d0200edd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42e07609569d4f8db4b3d5b52ab7d402",
      "placeholder": "​",
      "style": "IPY_MODEL_d924e4dcc3684991be8a7905c493e1aa",
      "value": " 456k/456k [00:00&lt;00:00, 760kB/s]"
     }
    },
    "7306b2f2693441dd98f6a7fa706493ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76390fe0464f4eeab83b4dc1130c025e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_482fc7efcdc64746afe7309010ac23ce",
      "placeholder": "​",
      "style": "IPY_MODEL_2eda110549e9406a831faf14c2b1d3ca",
      "value": " 665/665 [00:00&lt;00:00, 31.9kB/s]"
     }
    },
    "7791af86f6a44a5ca47721216b78c52b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c0635408d43442b828818e88dd4b43b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7791af86f6a44a5ca47721216b78c52b",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1e3173bf62c74f539c7dd776a287bf6b",
      "value": 665
     }
    },
    "7f3c65c0bb9544faa7f682c38a15249a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92a8c88c88ca4d4bb9f8d3e820da7886": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d8200500e584c27a764ef0dd9dbd904": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27b8a763070c4cbf8eff025dd1766704",
      "placeholder": "​",
      "style": "IPY_MODEL_f07be588da5e4346a0b495af794feb60",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "a76e6264d2a045caabc1831d136cc980": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab006f19113f4bd29863a2c5aa9c3142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1919a8349edf4e379a91e37f60fd2e5e",
      "placeholder": "​",
      "style": "IPY_MODEL_7f3c65c0bb9544faa7f682c38a15249a",
      "value": "Downloading (…)olve/main/merges.txt: 100%"
     }
    },
    "c1ffb6c52e354ce3888a67004091a066": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb32a5cd024f47f79bea73278badaf47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f9075bd71c748abbc8b0aab6ea9d638",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1da0cb6f46a84960918724301fabeb71",
      "value": 456318
     }
    },
    "d924e4dcc3684991be8a7905c493e1aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4a5d9c62c514530901bb74dadcc6008": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb616ed9d31a4f7d816a2dd0b33e9339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed4093c5330942958fe107f272b3f843": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f33084d07f64b6b8741ad8fd0e036ff",
       "IPY_MODEL_1510121315844ecca315061ed797ca55",
       "IPY_MODEL_edcef9a9db234e4391a1496589e57348"
      ],
      "layout": "IPY_MODEL_2d12da8b2b364a06a70615fdb9e0a8e6"
     }
    },
    "edcef9a9db234e4391a1496589e57348": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4a5d9c62c514530901bb74dadcc6008",
      "placeholder": "​",
      "style": "IPY_MODEL_3d00e43236d34dc58bade00be5feb1ca",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 1.29MB/s]"
     }
    },
    "f07be588da5e4346a0b495af794feb60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f43b6d5c769e4416b9969afbf9c77569": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
