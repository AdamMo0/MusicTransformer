{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58a6c05",
   "metadata": {},
   "source": [
    "# Predict abc notes based on 8 bars input using BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9a150d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adam/Desktop/Adam/Masters_Courses/Semster_3\n"
     ]
    }
   ],
   "source": [
    "from Preprocessing_notebook import read_abc\n",
    "from Transformer_model import  get_model\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import youtokentome as yttm\n",
    "from argparse import ArgumentParser\n",
    "import os \n",
    "ORIGIN = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "print(ORIGIN)\n",
    "RUN5_DIR = ORIGIN +\"/Music/MusicTransformer/Run_5/221394_134131.abc\"\n",
    "\n",
    "TOKENIZER_DIR = ORIGIN +\"/Music/maestro-v3.0.0/new_abc_notitle.yttm\"\n",
    "TOKENIZER_DIR = ORIGIN +\"/Music/maestro-v3.0.0/abc_newDS.yttm\"\n",
    "\n",
    "OUTPUT_DIR = ORIGIN +\"/Music/maestro-v3.0.0/output\"\n",
    "OUTPUT_PREDICT_DIR = ORIGIN +\"/Music/maestro-v3.0.0/output/predict\"\n",
    "model_path = \"/Users/adam/Desktop/Adam/Masters_Courses/Semster_3/Music/pytorch_model_FINAL_newDS.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3b209ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_paths = TEST_DIR\n",
    "test_paths = sorted(test_paths)\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = yttm.BPE(TOKENIZER_DIR) # import the trained tokenizer\n",
    "\n",
    "model = get_model(vocab_size=tokenizer.vocab_size()) # load the BERT model \n",
    "\n",
    "\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defdc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "USEABLE_PARAMS = [i+\":\" for i in \"BCDFGHIKLMmNOPQRrSsTUVWwXZ\"] # These are the parameters for key\n",
    "\n",
    "def read_abc(path):\n",
    "    keys = []\n",
    "    notes = []\n",
    "    with open(path) as rf:\n",
    "        for line in rf:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"%\"): # Skip any commments\n",
    "                continue\n",
    "\n",
    "            if any([line.startswith(key) for key in USEABLE_PARAMS]):\n",
    "                if(line.startswith('T')):\n",
    "                    continue # skipping the title for better tokenization\n",
    "                keys.append(line)\n",
    "            else:\n",
    "                notes.append(line)\n",
    "\n",
    "    keys = \" \".join(keys)\n",
    "\n",
    "    notes = \"\".join(notes).strip()\n",
    "    notes = notes.replace(\" \", \"\")\n",
    "\n",
    "    if notes.endswith(\"|\"):\n",
    "        notes = notes[:-1]\n",
    "    # Remove unneeded character.\n",
    "    notes = notes.replace(\" \\ \", \"\")\n",
    "    notes = notes.replace(\"\\\\\", \"\")\n",
    "    notes = notes.replace(\"\\ \", \"\")\n",
    "    notes = notes.replace(\"x8|\", \"\") # 8 because all of the midi file has a L:1/8 that means one muted bar\n",
    "    notes = notes.replace(\"z8|\", \"\") # 8 because all of the midi file has a L:1/8 that means one muted bar\n",
    "\n",
    "    notes = notes.strip()\n",
    "    notes = \" \".join(notes.split(\" \"))\n",
    "\n",
    "    if not keys or not notes:\n",
    "        return None, None\n",
    "\n",
    "    return keys, notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47693bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_notes(model, tokenizer, keys, notes):\n",
    "    print(notes)\n",
    "    keys_tokens = tokenizer.encode(keys)\n",
    "    notes_tokens = tokenizer.encode(notes)\n",
    "\n",
    "    if len(keys_tokens) + len(notes_tokens) > 510:\n",
    "        notes_tokens = notes_tokens[len(notes_tokens) - len(keys_tokens) - 510:]\n",
    "\n",
    "    context_tokens = [2] + keys_tokens + notes_tokens + [3]\n",
    "\n",
    "    context_tokens = torch.tensor(context_tokens, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "\n",
    "    \n",
    "    bad_words_ids = []\n",
    "    bad_words = [\"x8 | \"]\n",
    "    for w in bad_words:\n",
    "        bad_words_ids.append(tokenizer.encode(bad_words)[0])\n",
    "\n",
    "    gen_tokens = model.generate(input_ids=context_tokens, \n",
    "                            max_length=80, \n",
    "                            min_length=40,\n",
    "                            num_beams=70, # num of memorizing sequences\n",
    "                            bos_token_id=2, \n",
    "                            eos_token_id=3,\n",
    "                            no_repeat_ngram_size=2, # allows to avoid melody repeating\n",
    "                            pad_token_id=0,\n",
    "                            temperature = 0.5,\n",
    "                            top_k = 5,\n",
    "                                \n",
    "                            bad_words_ids=bad_words_ids)\n",
    "                                \n",
    "    gen_tokens = gen_tokens[0].tolist()\n",
    "\n",
    "    notes = tokenizer.decode(gen_tokens, ignore_ids=[0,1,2,3])[0]\n",
    "    notes = notes.replace(\" \", \"\").replace(\"|\", \"|\\n\")\n",
    "    \n",
    "    return notes\n",
    "\n",
    "def predict(model, tokenizer, text_path, output_dir):\n",
    "    text_path = TEST_DIR\n",
    "    files = os.listdir(text_path)\n",
    "\n",
    "    for i,file in enumerate(files):\n",
    "        print(file)\n",
    "        if(file == '.DS_Store'):\n",
    "            continue\n",
    "        keys, notes = read_abc(RUN5_DIR)\n",
    "        print(keys)\n",
    "        print(notes)\n",
    "        if notes is None:\n",
    "            print(keys)\n",
    "            continue\n",
    "            \n",
    "    #keys, notes = read_abc(text_path)\n",
    "    print(keys)\n",
    "\n",
    "\n",
    "    # Find the index of the 8th '|' character\n",
    "    index = 0\n",
    "    count = 0\n",
    "    for i, char in enumerate(notes):\n",
    "        if char == '|':\n",
    "            count += 1\n",
    "            if count == 8:\n",
    "                index = i\n",
    "                break\n",
    "\n",
    "# Remove the text after the 8th '|' character\n",
    "    orignal_notes = notes[index:]\n",
    "\n",
    "    notes = notes[:index+1]    \n",
    "    \n",
    "    new_path = text_path\n",
    "\n",
    "    predicted_tokens = predict_notes(model, tokenizer, keys, notes)\n",
    "\n",
    "\n",
    "    return [predicted_tokens, orignal_notes]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb423a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import Levenshtein\n",
    "\n",
    "def calculate_bleu_score(reference, generated):\n",
    "\n",
    "    reference_tokens = reference.split()\n",
    "    generated_tokens = generated.split()\n",
    "\n",
    "    reference_corpus = [reference_tokens]\n",
    "\n",
    "    # Calculate the BLEU score based on 1gram 2gram and 3gram\n",
    "    bleu_score_ngram = nltk.translate.bleu_score.corpus_bleu(reference_corpus, generated_tokens)\n",
    "    bleu_score_2ngram = nltk.translate.bleu_score.corpus_bleu(reference_corpus, generated_tokens, weights=(0.5, 0.5))\n",
    "    bleu_score_3grams = nltk.translate.bleu_score.corpus_bleu(reference_corpus, generated_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "    print(f\"1gram : {bleu_score_ngram}, 2grams: {bleu_score_2ngram}, 3grams{bleu_score_3grams}\")\n",
    "    return bleu_score\n",
    "\n",
    "def levenshtein(abc_1 ,abc_2):\n",
    "    original = abc_1\n",
    "    predicted = abc_2\n",
    "\n",
    "    # Calculate Levenshtein distance\n",
    "    distance = Levenshtein.distance(abc_1, abc_2)\n",
    "\n",
    "    # Normalize the distance to a similarity score\n",
    "    max_length = max(len(original), len(predicted))\n",
    "    similarity_score = 1 - (distance / max_length)\n",
    "\n",
    "    print(f\"Similarity Score: {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "35ddff44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/84 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      ".DS_Store\n",
      "7_162982.abc\n",
      "/Users/adam/Desktop/Adam/Masters_Courses/Semster_3/Music/maestro-v3.0.0/test_1_path/7_162982.abc\n",
      "X:1 M:4/4 L:1/8 Q:1/4=222 K:E%4sharps V:1\n",
      "A,,2-[e4A4E4A,,4]E,2-|[a2e2A2E,2]A,-[a3-e3-A3-A,3-][a/2e/2-A/2-A,/2E,/2-][e/2A/2-E,/2-][A/2E,/2-]E,/2|E,,2-[e3-B3-E3-E,,3-][e/2-B/2E/2E,,/2-][e/2E,,/2]B,,2-|[g3/2-e3/2-B3/2B,,3/2-][g/2e/2B,,/2]E,-[g-e-B-E,-][g/2-e/2-B/2-E,/2F,,/2-][g3/2e3/2B3/2-F,,3/2][B/2G,,/2-]G,,3/2|A,,2-[e4A4E4A,,4]E,2-|[a3/2e3/2-A3/2E,3/2-][e/2E,/2]A,-[a3-e3-A3-A,3][aeA-E,-][A/2E,/2-]E,/2|A,,2-[e4A4E4A,,4]E,2-|[a2e2A2E,2-][A,/2-E,/2]A,/2-[a3-e3-A3-A,3-][a/2-e/2-A/2-A,/2E,/2-][aeA-E,-][A/2E,/2]|E,,2-[e3-B3-E3-E,,3-][e/2-B/2E/2E,,/2-][e/2E,,/2]B,,2-|[g3/2-e3/2-B3/2B,,3/2-][g/2e/2B,,/2]E,-[g4e4B4-E,4-][B/2E,/2]x/2|B,,3-[b3-d3-F3-B,,3-][b/2-d/2-F/2-B,,/2B,,,/2-][b-d-FB,,,-][b/2d/2B,,,/2-]|[b3/2f3/2B3/2F3/2B,,,3/2]x6x/2|[B,,8-E,,8-]|[B,,4-E,,4-][e4B,,4-E,,4-]|[g3-B3-B,,3-E,,3-][g/2B/2B,,/2-E,,/2-][B,,/2-E,,/2-][g4-B4-B,,4-E,,4-]|[g6-B6-B,,6-E,,6-][g3/2B3/2B,,3/2-E,,3/2-][B,,/2E,,/2]\n",
      "X:1 M:4/4 L:1/8 Q:1/4=222 K:E%4sharps V:1\n",
      "A,,2-[e4A4E4A,,4]E,2-|[a2e2A2E,2]A,-[a3-e3-A3-A,3-][a/2e/2-A/2-A,/2E,/2-][e/2A/2-E,/2-][A/2E,/2-]E,/2|E,,2-[e3-B3-E3-E,,3-][e/2-B/2E/2E,,/2-][e/2E,,/2]B,,2-|[g3/2-e3/2-B3/2B,,3/2-][g/2e/2B,,/2]E,-[g-e-B-E,-][g/2-e/2-B/2-E,/2F,,/2-][g3/2e3/2B3/2-F,,3/2][B/2G,,/2-]G,,3/2|A,,2-[e4A4E4A,,4]E,2-|[a3/2e3/2-A3/2E,3/2-][e/2E,/2]A,-[a3-e3-A3-A,3][aeA-E,-][A/2E,/2-]E,/2|A,,2-[e4A4E4A,,4]E,2-|[a2e2A2E,2-][A,/2-E,/2]A,/2-[a3-e3-A3-A,3-][a/2-e/2-A/2-A,/2E,/2-][aeA-E,-][A/2E,/2]|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/84 [00:27<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted\n",
      "a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2[E-D-][E2D2C2][E-D-][E2D2C2]][c/2A/2F/2][c/2A/2F/2][c/2A/2F/2][c/2A/2F/2F2D2B,2][F2D2B,2][A2-E2-A,2-][A2-E2-A,2-][_G/2-E/2-B,/2][B/2-__G/2-E/2-B,/2][B/2-_G,3/2-_E,3/2-][G,3/2-_E,3/2-][,/2x/2[,/2x/2[g/2-d/2-G/2-D/2-B,/2G,/2-D,/2-B,,/2G,,/2-][g/2d/2G/2-D/2g/2-d/2-G/2-D/2-B,/2G,/2-D,/2-B,,/2G,,/2-][g/2d/2G/2-D/2G/2-E/2-=B,/2-E,,/2][G/2E/2=B,/2G/2-E/2-=B,/2-E,,/2][G/2E/2=B,/2[B3/2-G3/2-[B3/2-G3/2-ED-ED-^F,,3/2][^F,,3/2][[=D/2-A,/2-_G,/2-D,,/2][D/2-A,/2-_G,/2-A,,/2][D/2A,/2_G,/2D,/2]x/2[[=D/2-A,/2-_G,/2-D,,/2][D/2-A,/2-_G,/2-A,,/2][D/2A,/2_G,/2D,/2]x/2[F,2-C,2-][A,2F,2-C,2-][B,2F,2-C,2-][F,2-C,2-][A,2F,2-C,2-][B,2F,2-C,2-][A,/2][c'/2A,/2][c'/2G/2E/2C/2G,/2C,/2][G/2E/2C/2G,/2C,/2][A,/2-F,/2-C,/2F,,/2][A,/2-F,/2-C,/2F,,/2][d2B2E2d2B2E2c/2F/2C/2c/2F/2C/2E/2B,/2G,/2E,/2-E,,/2-][E,/2E,,/2-][C/2E,/2E,,/2]x/2[E/2B,/2G,/2E,/2-E,,/2-][E,/2E,,/2-][C/2E,/2E,,/2]x/2[^A,/2^G,/2F,,/2-][C/2F,,/2-][^D/2^C/2F,,/2-][F/2F,,/2-][^G/2^F/2=F,,/2-][^A/2F,,/2-][=c/2F,,/2-][^c/2F,,/2-][^d/2^A,/2^G,/2F,,/2-][C/2F,,/2-][^D/2^C/2F,,/2-][F/2F,,/2-][^G/2^F/2=F,,/2-][^A/2F,,/2-][=c/2F,,/2-][^c/2F,,/2-][^d/2_D,/2E,/2_D,/2E,/2A,2-A,,2-][A,2-A,,2-][[F/2D/2B,/2]x3/2[F/2D/2B,/2F,/2]xF,/2[F/2D/2B,/2]x[F/2D/2B,/2]x3/2[F/2D/2B,/2F,/2]xF,/2[F/2D/2B,/2]xE3C3B,3G,3E3C3B,3G,3[D/2-B,/2-][F3/2-D3/2B,3/2][B/2-F/2[D/2-B,/2-][F3/2-D3/2B,3/2][B/2-F/2A2^A2^[c6-=A[c6-=AB/2-]B/2x/2B/2-]B/2x/2D/2-B,/2-][D-B,-G,-D,-][G/2-D/2B,/2-G,/2D,/2][G/2-B,/2-][GD/2-B,/2-][D-B,-G,-D,-][G/2-D/2B,/2-G,/2D,/2][G/2-B,/2-][GB,/2-E,/2-][E/2-B,/2-E,/2-][G/2-E/2-B,/2E,/2-][G/2-E/2E,/2-][B,/2-E,/2-][E/2-B,/2-E,/2-][G/2-E/2-B,/2E,/2-][G/2-E/2E,/2-][e-c-A][e-c-A][][A/2A,/2][][A/2A,/2][cG-E-cG-E-[E-B,-E,-][[E-B,-E,-][]x/2[F/2C/2A,/2]x/2[]x/2[F/2C/2A,/2]x/2[D,-[F,/2D,/2][D,-[F,/2D,/2][A,/2-F,/2^D,/2]A,/2-F,/2^D,/2]\n",
      "orginal\n",
      "|E,,2-[e3-B3-E3-E,,3-][e/2-B/2E/2E,,/2-][e/2E,,/2]B,,2-|[g3/2-e3/2-B3/2B,,3/2-][g/2e/2B,,/2]E,-[g4e4B4-E,4-][B/2E,/2]x/2|B,,3-[b3-d3-F3-B,,3-][b/2-d/2-F/2-B,,/2B,,,/2-][b-d-FB,,,-][b/2d/2B,,,/2-]|[b3/2f3/2B3/2F3/2B,,,3/2]x6x/2|[B,,8-E,,8-]|[B,,4-E,,4-][e4B,,4-E,,4-]|[g3-B3-B,,3-E,,3-][g/2B/2B,,/2-E,,/2-][B,,/2-E,,/2-][g4-B4-B,,4-E,,4-]|[g6-B6-B,,6-E,,6-][g3/2B3/2B,,3/2-E,,3/2-][B,,/2E,,/2]\n",
      "0.457157589155434\n",
      "1gram : 0.01296645497158649, 2grams: 0.018404449158742876, 3grams0.01564047796758959\n",
      "BLEU Score: 0.5568104387716536\n",
      "Similarity Score: 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starts generation\")\n",
    "output_dir = Path(TEST_DIR)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "for p in tqdm(test_paths):\n",
    "    print(p)\n",
    "    abc_path = predict(model, tokenizer, p, output_dir)\n",
    "    print('predicted')\n",
    "    print(abc_path[0])\n",
    "    print('orginal')\n",
    "    print(abc_path[1])\n",
    "    \n",
    "    print(bars_similarity(str(abc_path[0]),str(abc_path[1])))\n",
    "    bleu_score = calculate_bleu_score(abc_path[0], abc_path[1])\n",
    "    print(\"BLEU Score:\", bleu_score)\n",
    "    levenshtein(abc_path[0],abc_path[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68f088a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/84 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      ".DS_Store\n",
      "7_162982.abc\n",
      "/Users/adam/Desktop/Adam/Masters_Courses/Semster_3/Music/maestro-v3.0.0/test_1_path/7_162982.abc\n",
      "X:1 M:4/4 L:1/8 Q:1/4=120 K:F%1flats V:1\n",
      "G,2x2x/2D2-D/2G-|G/2F/2-[G/2-F/2]G/2A/2G/2-[B/2-G/2]B/2x/2B,B,3/2B|x/2B>GB<GE3/2D2|ED3/2G-[G/2E/2]x/2G/2-[G/2E/2-][E/2D/2]x/2=B,/2-[D/2-=B,/2][D/2C/2-]|C/2=B,<G,G,,3/2G,3/2xD/2=B,-|=B,/2C2D<ED3-D/2|x/2E3D2<D,2D/2-|D_B3/2B,-[B/2B,/2]xGx/2G,3/2|G3/2E-[E/2D/2-]D3/2E/2x/2D3/2G-|[G/2E/2]x/2G/2-[G/2E/2-]E/2(3D=B,DC/2-[C/2=B,/2-]=B,/2G,-[G,/2G,,/2-]G,,/2-|G,,/2G,2=B,<ED/2x/2G/2-[G/2E/2-][E/2D/2]x/2=B,/2-|=B,C4Ex/2=B,3/2|(3D2C2=B,2(3G,2D2F2|Gx/2Ax/2(3_B2G2E2D-|DE(3D2G2E2D3/2C/2-|C(3=B,2G,2G,,2G,>DG\n",
      "X:1 M:4/4 L:1/8 Q:1/4=120 K:F%1flats V:1\n",
      "G,2x2x/2D2-D/2G-|G/2F/2-[G/2-F/2]G/2A/2G/2-[B/2-G/2]B/2x/2B,B,3/2B|x/2B>GB<GE3/2D2|ED3/2G-[G/2E/2]x/2G/2-[G/2E/2-][E/2D/2]x/2=B,/2-[D/2-=B,/2][D/2C/2-]|C/2=B,<G,G,,3/2G,3/2xD/2=B,-|=B,/2C2D<ED3-D/2|x/2E3D2<D,2D/2-|D_B3/2B,-[B/2B,/2]xGx/2G,3/2|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                           | 1/84 [00:23<32:12, 23.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F,/2B,,/2-]B,,/2F,/2B,,/2-]B,,/2_D,_D,C,-]C,C,-]C,E/2-A,,/2]E/2-[E/2-A,,/2]E/2-[A,E,A,,]xx/2x/2[CG,C,]x/2x/2[A,E,A,,]xx/2x/2[CG,C,]x/2x/2[E/2-C/2-A,/2E,/2-A,,/2-][E/2-C/2-E,/2-A,,/2-][E/2-C/2-A,/2E,/2-A,,/2-][E/2-C/2-E,/2-A,,/2-][(3ddd(3dddd2B2][d2B2][F-A,-F,,][F-A,-F,,][C,,/2-]C,C,,/2-]C,_E-C-G,-_E-C-G,-B,E,B,E,^F^D^F^D=E/2x/2[=E/2x/2[[GEC[GEC[D/2B,/2][F/2D/2]B/2[F/2D/2][B/2F/2][d/2B/2[D/2B,/2][F/2D/2]B/2[F/2D/2][B/2F/2][d/2B/2e/2c/2]x/2[e/2c/2e/2c/2]x/2[e/2c/2g/2f/2e/2d/2g/2f/2e/2d/2C,/2]x3C,/2]x3[^G-EB,]^G/2[E/2B,/2^G,/2][^G-EB,]^G/2[E/2B,/2^G,/2][A-EC]A/2[[^G-EB,]^G/2[E/2B,/2^G,/2][^G-EB,]^G/2[E/2B,/2^G,/2][A-EC]A/2[][f/2B/2][f/2B/2^F/2D/2^F/2D/2d/2B/2G/2-D/2-B,/2-G,/2-E,/2-][d/2B/2G/2-D/2-B,/2-G,/2-E,/2-][G/2-E/2-B,/2][G/2-E/2-B,/2][^D,/2]x[^D,/2]x[G2C2A,2G2C2A,2D3-B,3-][D3-B,3-][[d3A3F3]x/2[[d3A3F3]x/2[C/2A,/2F,/2-][C/2A,/2F,/2-][B,/2-G,/2-G,,/2-][B,/2-G,/2-G,,/2-][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][F/2C/2A,/2][G-D-B,-G,-D,-B,,-G,,-][G-D-B,-G,-D,-B,,-G,,-][_A,4_A,4F3/2F3/2[E2C2G,2][E2C2G,2][[E2C2G,2][E2C2G,2][G3D3G3D3[c/2E/2[c/2E/2=E/2C/2=E/2C/2A,,A,,\n",
      "========\n",
      "|G3/2E-[E/2D/2-]D3/2E/2x/2D3/2G-|[G/2E/2]x/2G/2-[G/2E/2-]E/2(3D=B,DC/2-[C/2=B,/2-]=B,/2G,-[G,/2G,,/2-]G,,/2-|G,,/2G,2=B,<ED/2x/2G/2-[G/2E/2-][E/2D/2]x/2=B,/2-|=B,C4Ex/2=B,3/2|(3D2C2=B,2(3G,2D2F2|Gx/2Ax/2(3_B2G2E2D-|DE(3D2G2E2D3/2C/2-|C(3=B,2G,2G,,2G,>DG\n",
      ".\n",
      ".DS_Store\n",
      "7_162982.abc\n",
      "/Users/adam/Desktop/Adam/Masters_Courses/Semster_3/Music/maestro-v3.0.0/test_1_path/7_162982.abc\n",
      "X:1 M:4/4 L:1/8 Q:1/4=120 K:F%1flats V:1\n",
      "G,2x2x/2D2-D/2G-|G/2F/2-[G/2-F/2]G/2A/2G/2-[B/2-G/2]B/2x/2B,B,3/2B|x/2B>GB<GE3/2D2|ED3/2G-[G/2E/2]x/2G/2-[G/2E/2-][E/2D/2]x/2=B,/2-[D/2-=B,/2][D/2C/2-]|C/2=B,<G,G,,3/2G,3/2xD/2=B,-|=B,/2C2D<ED3-D/2|x/2E3D2<D,2D/2-|D_B3/2B,-[B/2B,/2]xGx/2G,3/2|G3/2E-[E/2D/2-]D3/2E/2x/2D3/2G-|[G/2E/2]x/2G/2-[G/2E/2-]E/2(3D=B,DC/2-[C/2=B,/2-]=B,/2G,-[G,/2G,,/2-]G,,/2-|G,,/2G,2=B,<ED/2x/2G/2-[G/2E/2-][E/2D/2]x/2=B,/2-|=B,C4Ex/2=B,3/2|(3D2C2=B,2(3G,2D2F2|Gx/2Ax/2(3_B2G2E2D-|DE(3D2G2E2D3/2C/2-|C(3=B,2G,2G,,2G,>DG\n",
      "X:1 M:4/4 L:1/8 Q:1/4=120 K:F%1flats V:1\n",
      "G,2x2x/2D2-D/2G-|G/2F/2-[G/2-F/2]G/2A/2G/2-[B/2-G/2]B/2x/2B,B,3/2B|x/2B>GB<GE3/2D2|ED3/2G-[G/2E/2]x/2G/2-[G/2E/2-][E/2D/2]x/2=B,/2-[D/2-=B,/2][D/2C/2-]|C/2=B,<G,G,,3/2G,3/2xD/2=B,-|=B,/2C2D<ED3-D/2|x/2E3D2<D,2D/2-|D_B3/2B,-[B/2B,/2]xGx/2G,3/2|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                         | 1/84 [00:47<1:05:03, 47.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2G/2-E/2-C/2-C,/2][G/2E/2C/2G/2-E/2-C/2-C,/2][G/2E/2C/2]F,,]F,,G-E-C][G-E-C][f/2d/2=A/2f/2d/2=A/2D3/2-A,3/2-D3/2-A,3/2-=d/2F/2-A,/2-][F/2-A,/2-][a/2g/2f/2=d/2F/2-A,/2-][F/2-A,/2-][a/2g/2f/2B/2=F/2B/2=F/2[AEC[AECG,/2E,/2C,/2][G,/2E,/2C,/2][G,/2E,/2C,/2][G,/2E,/2C,/2][B,,/2-][E/2B,,/2-][B,,/2-][E/2B,,/2-][D,/2]A/2-[A/2-D,/2]A/2-[A/2-,/2x/2[,/2x/2[F/2D/2B,/2F,/2][F/2D/2B,/2F/2D/2B,/2F,/2][F/2D/2B,/2g/2G/2g/2G/2F-F,-][F-F,-][GECG,-C,-][GECG,-C,-][F,/2-]F,/2-F,/2-]F,/2-E2-C2-A,2-][E/2C/2A,/2][E2-C2-A,2-][E/2C/2A,/2][=C/2A,/2=C/2A,/2C2-A,2-F,2-][C2-A,2-F,2-][c/2G/2E/2A,/2-]A,/2c/2G/2E/2A,/2-]A,/2[D2B,2G,2E,2A,,2]x[D3/2B,3/2G,3/2A,,3/2]x/2[D/2B,/2G,/2A,,/2]x/2[D2B,2G,2[D2B,2G,2E,2A,,2]x[D3/2B,3/2G,3/2A,,3/2]x/2[D/2B,/2G,/2A,,/2]x/2[D2B,2G,2C2-C/2C2-C/2D/2_A,/2D/2_A,/2AE^CAE^C[E2C2A,2[E2C2A,2B-B-G3-F3-D3-][G3-F3-D3-][g3-g3-[AFC-F,-][[AFC-F,-][[E3/2[E3/2d/2B/2E/2]x[d/2B/2E/2]x3/2[d/2B/2E/2]x3/2[d/2B/2E/2d/2B/2E/2]x[d/2B/2E/2]x3/2[d/2B/2E/2]x3/2[d/2B/2E/2a/2e/2-a/2e/2-A3-A/2x/2A3-A/2x/2A/2A,/2D,/2-G,,/2-][D,/2-G,,/2-][A/2A,/2D,/2-G,,/2-][D,/2-G,,/2-][C2-C,2-][C2-C,2-][x2[F,3x2[F,3][b/2][b/2\n",
      "========\n",
      "|G3/2E-[E/2D/2-]D3/2E/2x/2D3/2G-|[G/2E/2]x/2G/2-[G/2E/2-]E/2(3D=B,DC/2-[C/2=B,/2-]=B,/2G,-[G,/2G,,/2-]G,,/2-|G,,/2G,2=B,<ED/2x/2G/2-[G/2E/2-][E/2D/2]x/2=B,/2-|=B,C4Ex/2=B,3/2|(3D2C2=B,2(3G,2D2F2|Gx/2Ax/2(3_B2G2E2D-|DE(3D2G2E2D3/2C/2-|C(3=B,2G,2G,,2G,>DG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starts generation\")\n",
    "output_dir = Path(TEST_DIR)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "counter = 0\n",
    "for p in tqdm(test_paths):\n",
    "    print(p)\n",
    "    results = predict(model, tokenizer, p, output_dir)\n",
    "    print(results[0])\n",
    "    print(\"========\")\n",
    "    print(results[1])\n",
    "    counter =counter+1\n",
    "    if counter ==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ef095caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1gram : 0.38582220354401836, 2grams: 0.4881704186809272, 3grams0.4371174935203348\n",
      "BLEU Score: 0.5568104387716536\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def calculate_bleu_score(reference, generated):\n",
    "    # Tokenize the reference and generated sequences\n",
    "    reference_tokens = reference.split()\n",
    "    generated_tokens = generated.split()\n",
    "\n",
    "    # Create a list of reference token lists (BLEU expects a list of lists)\n",
    "    reference_corpus = [reference_tokens]\n",
    "\n",
    "    # Calculate the BLEU score\n",
    "    bleu_score_ngram = nltk.translate.bleu_score.corpus_bleu(reference_corpus, generated_tokens)\n",
    "    bleu_score_2ngram = nltk.translate.bleu_score.corpus_bleu(reference_corpus, generated_tokens, weights=(0.5, 0.5))\n",
    "    bleu_score_3grams = nltk.translate.bleu_score.corpus_bleu(reference_corpus, generated_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "    print(f\"1gram : {bleu_score_ngram}, 2grams: {bleu_score_2ngram}, 3grams{bleu_score_3grams}\")\n",
    "    return bleu_score\n",
    "\n",
    "# Example usage\n",
    "#215688_124450\n",
    "# abc_1 =\"|[_A2-G2-D2-A,2][AGD-]D/2x3/2[F/2D/2_B,/2]x3/2[F/2D/2B,/2]x/2|x2[G3/2E3/2B,3/2]x3/2[F/2D/2B,/2]x3/2[F/2D/2B,/2]x/2|x2[G3/2E3/2B,3/2]x3/2[F/2D/2B,/2-]B,/2x[F/2D/2B,/2]x/2|x2[G3/2E3/2B,3/2]x3/2[F/2D/2-B,/2]D/2x[F/2D/2B,/2]x/2|x2[GEB,]x2[C/2A,/2F,/2]x3/2[C/2A,/2F,/2]x/2|x2[D3/2B,3/2F,3/2]x3/2[C/2A,/2F,/2]x3/2[C/2A,/2F,/2]x/2|x2[D3/2B,3/2F,3/2]x3/2[C/2A,/2F,/2]x3/2[C/2A,/2F,/2]x/2|x2[D3/2B,3/2F,3/2]x3/2[C/2A,/2F,/2]x3/2[C/2A,/2F,/2]x/2\"\n",
    "# abc_2_bert = \"F,/2B,,/2-]B,,/2F,/2B,,/2-]B,,/2D/2-G,/2-D/2-G,/2-E2][dD][cC][E2][dD][cC][G,/2-]G,/2-G,/2-]G,/2-c/2-A/2-F/2-C/2-A,/2c/2-A/2-F/2-C/2-A,/2B,,/2E,,/2][B,,E,,][B,,E,,][B,,/2E,,/2]E,/2B,,/2E,,/2][B,,E,,][B,,E,,][B,,/2E,,/2]E,/2^d/2-=^d/2-=A,4D,4][A,4D,4][=B,-F,-][=B,-F,-][f/2f/2f/2d/2f/2f/2f/2f/2f/2d/2f/2f/2E,C,E,C,B,/2G,/2E,/2C,/2-]C,/2[B,/2G,/2E,/2C,/2-]C,/2[B,/2G,/2E,/2C,/2-]C,/2[B,/2-G,/2E,/2C,/2]B,/2G,/2E,/2C,/2-]C,/2[B,/2G,/2E,/2C,/2-]C,/2[B,/2G,/2E,/2C,/2-]C,/2[B,/2-G,/2E,/2C,/2]E,/2A,,/2]x/2[E,/2A,,/2]x/2[F,[\"\n",
    "\n",
    "# abc_2 =\"x2[DB,F,]x2B,-[DB,]F-|[f2-d2-B2-F2][f2-d2-B2-][f/2d/2-B/2-][d/2B/2]A,-[=B,A,]E-|[e2-=B2-A2-E2-][e/2-=B/2-A/2-E/2][e=B-A-][=B/2A/2]xE,-[G,-E,][=A,G,]|[c4-=A4-G4-E4-C4-][c=A-G-E-C][=A/2G/2E/2D,/2-]D,/2-[G,D,-][_A,/2-D,/2]A,/2-|[A2-G2-D2-A,2][A3/2G3/2D3/2]x3/2_B,-[D-B,-][F-D-B,-]|[f3-d3-B3-F3-D3-B,3][f/2d/2B/2F/2D/2]x4x/2|A,,-[_D-A,,-][E-_D-A,,-][A2-E2-_D2-A,,2-][A/2-E/2-_D/2-A,,/2][A3/2-E3/2-_D3/2][A-E-]|A/2-E/2=D,/2-][A/2-D,/2-][A-F,-D,-][A-A,-F,D,][A-A,-][A-D-A,-][A-F-DA,][A/2F/2]x/2|A,,-[F,-A,,-][A,-F,-A,,-][d-AFA,-F,-A,,-][d/2A,/2-F,/2-A,,/2-][A,/2-F,/2A,,/2][A\"\n",
    "#234675_21548\n",
    "# abc_1 = \"|[E4-C4-A,4-][ECA,-][E2A,2-][EA,]|[E8-B,8-A,8-]|[E4-B,4A,4-][E-A,-][ECA,-][EA,-][GA,]|[B3E3-A,3-][A3E3-A,3-][B2E2-A,2-]|[c4-=B4-_B4-E4-A,4-][c=B_BE-A,-][BE-A,-][AE-A,-][fEA,]|[e8-E8-A,8-]|[e8E8A,8]|x[eE][e4-E4-][eE][B-B,-]\"\n",
    "# abc_bert_2 = \"F,/2B,,/2-]B,,/2F,/2B,,/2-]B,,/2_D,_D,B,/2G,/2E,,/2-]E,,/2-[B,/2G,/2E,,/2-]E,,/2-[e/2-c/2-G/2-C/2][e/2c/2G/2e/2-c/2-G/2-C/2][e/2c/2G/2A-DA-D=CA,=CA,^G/2^G,/2-][^G/2^G,/2-][_E2C2_E2C2E3/2B,3/2G,3/2-E,3/2-B,,3/2-][E3/2B,3/2G,3/2-E,3/2-B,,3/2-][[c6-[c6-E^CG,-D,-G,,-][E^CG,-D,-G,,-][F,6F,6F,2C,2-F,,2][F,2C,2-F,,2][6G,6-6G,6-[B2-B,2-^G,2-][B-B,-^G,-][B-B,^G,][B-DB,]B-[B-D-B,-][BDB,-[B2-B,2-^G,2-][B-B,-^G,-][B-B,^G,][B-DB,]B-[B-D-B,-][BDB,-[ge[ge][e/2E/2][e/2E/2CG,-B,,][CG,-B,,][C/2A,/2F,/2-]F,/2[C/2A,/2F,/2-]F,/2[C/2-A,/2-E,/2-][C/2A,/2E,/2][C/2-A,/2-E,/2-][C/2A,/2E,/2][G/2D/2-B,/2G/2D/2-B,/2[B,3F,3-B,,3][[B,3F,3-B,,3][e-cAe-cAE,-][eE,-][eE/2-C/2-E,,/2-][E/2-C/2-E,,/2-][^A,,/2-][F/2^A,,/2-]\"\n",
    "# abc_gpt2_2 = \"E8-C2-C-C/2-C/2-C/2F,/2-][C/2-F,/2-]|C/2-F-C/2-F,/2-F,/2-][F-CF,][F2-C2-F,2-][F/2-C/2-F,/2][F/2-C/2G,/2-][F/2-G,/2-][F/2A,/2-G,/2F,/2-][A,/2-F,/2][A,/2-G,/2-]|A,-[C-A,-][F-C-A,-][c/2-F/2-C/2-A,/2][c/2-F/2C/2][c/2G,/2-]G,3/2-[D-G,-\"\n",
    "\n",
    "#106429_180318\n",
    "# abc_1 = \"|[A2-F2-D2-D,,2-][A/2-F/2-D/2-D,,/2][A/2-F/2-D/2-][A2-F2-D2-D,,2][A3-F3-D3-]|[A3/2-F3/2-E3/2D3/2-B,,,3/2-][A/2-F/2-D/2-B,,,/2][A-F-D-][A3/2-F3/2-D3/2-B,,,3/2][A2-F2-D2-][A/2-F/2-D/2-][A-F-DB,,,-]|[A/2-F/2E/2-C/2-B,,,/2A,,,/2-][A3/2-E3/2-C3/2-A,,,3/2-][A/2-F/2-E/2-C/2-A,,,/2][A/2-F/2E/2-C/2-][A2-E2-C2-A,,,2][A-E-C-A,-][A/2-E/2-D/2-C/2-A,/2][A/2-E/2-D/2C/2-][A-E-C-]|[A/2-F/2-E/2C/2-F,,/2-][A3/2-F3/2-C3/2-F,,3/2][A-F-C-G,,-][A/2-F/2-C/2-A,,/2-G,,/2][A/2-F/2-C/2-A,,/2-][A-GF-EC-A,,][A-F-C-C,][A3/2F3/2C3/2F,3/2]x/2|[A2-F2-D2-D,,2-][A/2-F/2-D/2-D,,/2][A/2-F/2-D/2-][\"\n",
    "# abc_2 = \"F,/2B,,/2-]B,,/2F,/2B,,/2-]B,,/2d/2-A/2-F/2-D/2-d/2-A/2-F/2-D/2-A2F2C2A2F2C2A3/2F3/2-D3/2-A,3/2-A,,3/2-][A3/2F3/2-D3/2-A,3/2-A,,3/2-][G/2-B,/2G,/2][G/2-B,/2G,/2][E3/2C3/2][E3/2C3/2][[A2F2D2A,2]F,[A2F2D2A,2]F,G,2E,2][G,2E,2][B/2G,/2-]G,/2-B/2G,/2-]G,/2-B/2-G/2-D/2-][B/2G/2D/2B/2-G/2-D/2-][B/2G/2D/2AF,-F,,-][cAF,-F,,-][cB,,/2-][A/2B,,/2-][B,,/2-][A/2B,,/2-][f2c2A2F2C2f2c2A2F2C2G/2-B,/2-G,/2G/2-B,/2-G,/2A,-F,-C,-F,,-][A,-F,-C,-F,,-][=G,,/2-]G,,/2x3=G,,/2-]G,,/2x3,/2B,,,,/2B,,,,/2B,,,,/2B,,,^AGEC-G,-C,-][=A^FDC-G,-C,-][^AGCG,C,][^AGEC-G,-C,-][=A^FDC-G,-C,-][^AGCG,C,]\"\n",
    "# abc_2 = \"F,,,-[B/2F/2C/2F,,,/2]x/2[B/2F/2C/2F,,,/2-]F,,,/2-[B/2F/2C/2F,,,/2]x/2[B-F-C][B/2F/2]x/2[AFC-]C/2x/2|[F-DB,F,,,-][F/2F,,,/2]x/2[FDB,F,,,-]F,,,/2x/2[F-D-B,][F/2-D/2C,,/2][F/2D,,/2][G/2-E/2-C/2-F,,/2-][G/2-E/2-C/2F,,/2D,,/2][G/2E/2C,,/2]A,,,/2|F,,,-[B/2F/2C/2F,,,/2]x/2[B/2F/2C/2F,,,/2-]F,,,/2-[B/2F/2C/2F,,,/2]x/2[B-F-C][B/2F/2]x/2[A-FC-][A/2C/2]x/2|[F-D-B,D,,-][F/2D/2D,,/2-]D,,/2-[F-DB,D,,-][F/2D,,/2-][D/2D,,/2][F-D-B,C,,-][A/2-F/2D/2C,,/2-][A/2C,,/2-][G-ECC,,-][G/2F/2-C,,/2-][F/2C,,/2]|[B-F-DB,,,-][B/2F/2B,,,/2-]B,,,/2-[G/2-F/2D/2B,,,/2]G/2[B/2-F/2-D/2-B,,,/2][B/2-F/2-\"\n",
    "#191238_36222\n",
    "\n",
    "#221394_134131\n",
    "abc_1 = \"|E,,2-[e3-B3-E3-E,,3-][e/2-B/2E/2E,,/2-][e/2E,,/2]B,,2-|[g3/2-e3/2-B3/2B,,3/2-][g/2e/2B,,/2]E,-[g4e4B4-E,4-][B/2E,/2]x/2|B,,3-[b3-d3-F3-B,,3-][b/2-d/2-F/2-B,,/2B,,,/2-][b-d-FB,,,-][b/2d/2B,,,/2-]|[b3/2f3/2B3/2F3/2B,,,3/2]x6x/2|[B,,8-E,,8-]|[B,,4-E,,4-][e4B,,4-E,,4-]|[g3-B3-B,,3-E,,3-][g/2B/2B,,/2-E,,/2-][B,,/2-E,,/2-][g4-B4-B,,4-E,,4-]|[g6-B6-B,,6-E,,6-][g3/2B3/2B,,3/2-E,,3/2-][B,,/2E,,/2]\"\n",
    "abc_2 = \"a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2a/2e/2]x/2[d'/2[E-D-][E2D2C2][E-D-][E2D2C2]][c/2A/2F/2][c/2A/2F/2][c/2A/2F/2][c/2A/2F/2F2D2B,2][F2D2B,2][A2-E2-A,2-][A2-E2-A,2-][_G/2-E/2-B,/2][B/2-__G/2-E/2-B,/2][B/2-_G,3/2-_E,3/2-][G,3/2-_E,3/2-][,/2x/2[,/2x/2[g/2-d/2-G/2-D/2-B,/2G,/2-D,/2-B,,/2G,,/2-][g/2d/2G/2-D/2g/2-d/2-G/2-D/2-B,/2G,/2-D,/2-B,,/2G,,/2-][g/2d/2G/2-D/2G/2-E/2-=B,/2-E,,/2][G/2E/2=B,/2G/2-E/2-=B,/2-E,,/2][G/2E/2=B,/2[B3/2-G3/2-[B3/2-G3/2-ED-ED-^F,,3/2][^F,,3/2][[=D/2-A,/2-_G,/2-D,,/2][D/2-A,/2-_G,/2-A,,/2][D/2A,/2_G,/2D,/2]x/2[[=D/2-A,/2-_G,/2-D,,/2][D/2-A,/2-_G,/2-A,,/2][D/2A,/2_G,/2D,/2]x/2[F,2-C,2-][A,2F,2-C,2-][B,2F,2-C,2-][F,2-C,2-][A,2F,2-C,2-][B,2F,2-C,2-][A,/2][c'/2A,/2][c'/2G/2E/2C/2G,/2C,/2][G/2E/2C/2G,/2C,/2][A,/2-F,/2-C,/2F,,/2][A,/2-F,/2-C,/2F,,/2][d2B2E2d2B2E2c/2F/2C/2c/2F/2C/2E/2B,/2G,/2E,/2-E,,/2-][E,/2E,,/2-][C/2E,/2E,,/2]x/2[E/2B,/2G,/2E,/2-E,,/2-][E,/2E,,/2-][C/2E,/2E,,/2]x/2[^A,/2^G,/2F,,/2-][C/2F,,/2-][^D/2^C/2F,,/2-][F/2F,,/2-][^G/2^F/2=F,,/2-][^A/2F,,/2-][=c/2F,,/2-][^c/2F,,/2-][^d/2^A,/2^G,/2F,,/2-][C/2F,,/2-][^D/2^C/2F,,/2-][F/2F,,/2-][^G/2^F/2=F,,/2-][^A/2F,,/2-][=c/2F,,/2-][^c/2F,,/2-][^d/2_D,/2E,/2_D,/2E,/2A,2-A,,2-][A,2-A,,2-][[F/2D/2B,/2]x3/2[F/2D/2B,/2F,/2]xF,/2[F/2D/2B,/2]x[F/2D/2B,/2]x3/2[F/2D/2B,/2F,/2]xF,/2[F/2D/2B,/2]xE3C3B,3G,3E3C3B,3G,3[D/2-B,/2-][F3/2-D3/2B,3/2][B/2-F/2[D/2-B,/2-][F3/2-D3/2B,3/2][B/2-F/2A2^A2^[c6-=A[c6-=AB/2-]B/2x/2B/2-]B/2x/2D/2-B,/2-][D-B,-G,-D,-][G/2-D/2B,/2-G,/2D,/2][G/2-B,/2-][GD/2-B,/2-][D-B,-G,-D,-][G/2-D/2B,/2-G,/2D,/2][G/2-B,/2-][GB,/2-E,/2-][E/2-B,/2-E,/2-][G/2-E/2-B,/2E,/2-][G/2-E/2E,/2-][B,/2-E,/2-][E/2-B,/2-E,/2-][G/2-E/2-B,/2E,/2-][G/2-E/2E,/2-][e-c-A][e-c-A][][A/2A,/2][][A/2A,/2][cG-E-cG-E-[E-B,-E,-][[E-B,-E,-][]x/2[F/2C/2A,/2]x/2[]x/2[F/2C/2A,/2]x/2[D,-[F,/2D,/2][D,-[F,/2D,/2][A,/2-F,/2^D,/2]A,/2-F,/2^D,/2]\"\n",
    "abc_2 = \"A,,2-[e4A4E4A,,4]E,2-|[a2e2A2E,2]A,-[a3-e3-A3-A,3-][a/2e/2-A/2-A,/2E,/2-][e/2A/2-E,/2-][A/2E,/2-]E,/2|E,,2-[e3-B3-E3-E,,3-][e/2-B/2E/2E,,/2-][e/2E,,/2]B,,2-|[g3/2-e3/2-B3/2B,,3/2-][g/2e/2B,,/2]E,-[g-e-B-E,-][g/2-e/2-B/2-E,/2F,,/2-][g3/2e3/2B3/2-F,,3/2][B/2G,,/2-]G,,3/2|A,,2-[e4A4E4A,,4]E,2-|[a3/2e3/2-A3/2E,3/2-][e/2E,/2]A,-[a3-e3-A3-A,3][aeA-E,-][A/2E,/2-]E,/2|A,,2-[e4A4E4A,,4]E,2-|[a2e2A2E,2-][A,/2-E,/2]A,/2-[a3-e3-A3-A,3-][a/2-e/2-A/2-A,/2E,/2-][aeA-E,-][A/2E,/2]|E,,2-[e3/2-B3/2-E3/2-E,,3/2][e/2B/2-E/2][B/2B,,/2-]B,,3/2-[g-e-B-B,,][geB-E]|B/2[E,/2A,,/2-]A,,-[e4-A4-E4-A,,4-A,,4-][e,,4-][e4-]|e/2-A,,4-]e/2-A,,/2-A,,4-]|e/2-]e/2-A-A-]e/2-A-A/2-A-A-A-E,/2-A-A-A-e/2-\"\n",
    "# Calculate BLEU score\n",
    "bleu_score = calculate_bleu_score(abc_1, abc_2)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9e85a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein(abc_1 ,abc_2):\n",
    "# Original and predicted ABC notation phrases\n",
    "    original = abc_1\n",
    "    predicted = abc_2\n",
    "\n",
    "    # Calculate Levenshtein distance\n",
    "    distance = Levenshtein.distance(abc_1, abc_2)\n",
    "\n",
    "    # Normalize the distance to a similarity score\n",
    "    max_length = max(len(original), len(predicted))\n",
    "    similarity_score = 1 - (distance / max_length)\n",
    "\n",
    "    print(f\"Similarity Score: {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2311bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.39\n"
     ]
    }
   ],
   "source": [
    "levenshtein(abc_1,abc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d2f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cabde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
